# Prometheus Alert Rules for Bike Traffic MLOps
# Defines thresholds and conditions for alerting

groups:
  - name: model_performance
    interval: 30s
    rules:
      # Critical: Model R² below 0.65 (reactive threshold)
      - alert: ModelPerformanceCritical
        expr: bike_model_r2_production < 0.65
        for: 5m
        labels:
          severity: critical
          component: model
        annotations:
          summary: "Model R² critically low (< 0.65)"
          description: "Production model R² is {{ $value | humanize }} which is below critical threshold of 0.65. Immediate retraining required."

      # Warning: Model R² below 0.70 (proactive threshold)
      - alert: ModelPerformanceWarning
        expr: bike_model_r2_production < 0.70 and bike_model_r2_production >= 0.65
        for: 10m
        labels:
          severity: warning
          component: model
        annotations:
          summary: "Model R² declining (< 0.70)"
          description: "Production model R² is {{ $value | humanize }}. Consider proactive retraining if drift is high."

      # High RMSE
      - alert: ModelRMSEHigh
        expr: bike_model_rmse_production > 60
        for: 5m
        labels:
          severity: critical
          component: model
        annotations:
          summary: "Model RMSE above threshold (> 60)"
          description: "Production model RMSE is {{ $value | humanize }} which exceeds acceptable threshold of 60."

  - name: data_drift
    interval: 30s
    rules:
      # High drift detected
      - alert: HighDriftDetected
        expr: bike_drift_share > 0.5
        for: 15m
        labels:
          severity: warning
          component: drift
        annotations:
          summary: "High data drift detected (> 50%)"
          description: "Drift share is {{ $value | humanizePercentage }}. {{ $labels.drifted_features_count }} features have drifted."

      # Critical drift with declining performance
      - alert: CriticalDriftWithDecliningPerformance
        expr: (bike_drift_share > 0.5) and (bike_model_r2_production < 0.70)
        for: 5m
        labels:
          severity: critical
          component: drift
        annotations:
          summary: "Critical drift + declining R² - proactive retraining needed"
          description: "Drift share: {{ $value | humanizePercentage }}, R²: {{ $labels.r2 }}. Immediate action required."

  - name: api_health
    interval: 15s
    rules:
      # High error rate
      - alert: APIErrorRateHigh
        expr: rate(fastapi_errors_total[5m]) / rate(fastapi_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "API error rate above 5%"
          description: "{{ $labels.endpoint }} has {{ $value | humanizePercentage }} error rate over the last 5 minutes."

      # High prediction latency
      - alert: PredictionLatencyHigh
        expr: histogram_quantile(0.95, rate(prediction_latency_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Prediction latency p95 > 5s"
          description: "95th percentile prediction latency is {{ $value | humanizeDuration }} which is above acceptable threshold."

      # Training timeout/failure
      - alert: TrainingFailure
        expr: increase(training_runs_total{status="failed"}[1h]) > 0
        for: 1m
        labels:
          severity: critical
          component: training
        annotations:
          summary: "Training run failed"
          description: "{{ $value }} training run(s) failed in the last hour. Check logs for details."

  - name: airflow_health
    interval: 30s
    rules:
      # DAG failures
      - alert: AirflowDAGFailed
        expr: increase(airflow_task_failures_total[30m]) > 0
        for: 5m
        labels:
          severity: warning
          component: airflow
        annotations:
          summary: "Airflow task failure detected"
          description: "DAG {{ $labels.dag_id }} task {{ $labels.task_id }} failed. Check Airflow UI."

      # Long-running DAG
      - alert: AirflowDAGTooSlow
        expr: airflow_dag_run_duration_seconds{dag_id="monitor_and_fine_tune"} > 3600
        for: 10m
        labels:
          severity: warning
          component: airflow
        annotations:
          summary: "DAG monitor_and_fine_tune taking > 1 hour"
          description: "DAG {{ $labels.dag_id }} has been running for {{ $value | humanizeDuration }}. Possible stuck task."

  - name: data_ingestion
    interval: 30s
    rules:
      # No data ingested
      - alert: NoDataIngested
        expr: increase(bike_records_ingested_total[1h]) == 0
        for: 2h
        labels:
          severity: warning
          component: ingestion
        annotations:
          summary: "No bike data ingested in 2 hours"
          description: "Data ingestion DAG may be failing or no new data available from Paris Open Data API."

      # Unusually low ingestion
      - alert: LowDataIngestion
        expr: increase(bike_records_ingested_total[1h]) < 100
        for: 2h
        labels:
          severity: info
          component: ingestion
        annotations:
          summary: "Low data ingestion rate"
          description: "Only {{ $value }} records ingested in the last hour. Verify API availability."
