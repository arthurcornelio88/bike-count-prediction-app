# üóÑÔ∏è BigQuery Setup & Secrets Configuration

**Objectif** : Configurer BigQuery et Google Secret Manager pour le pipeline Airflow MLOps

---

## üìã Table des mati√®res

1. [Architecture BigQuery](#architecture-bigquery)
2. [Cr√©ation des Datasets BigQuery](#cr√©ation-des-datasets-bigquery)
3. [Configuration Secret Manager](#configuration-secret-manager)
4. [Variables d'environnement locales (DEV)](#variables-denvironnement-locales-dev)
5. [Validation de la configuration](#validation-de-la-configuration)

---

## Architecture BigQuery

**3 Datasets pour tra√ßabilit√© compl√®te** :

```
datascientest-460618
‚îú‚îÄ‚îÄ bike_traffic_raw           # Donn√©es brutes quotidiennes
‚îÇ   ‚îî‚îÄ‚îÄ daily_YYYYMMDD        # Tables par jour
‚îÇ
‚îú‚îÄ‚îÄ bike_traffic_predictions   # Pr√©dictions quotidiennes
‚îÇ   ‚îî‚îÄ‚îÄ daily_YYYYMMDD        # Pr√©dictions + m√©triques
‚îÇ
‚îî‚îÄ‚îÄ monitoring_audit           # Logs de monitoring
    ‚îî‚îÄ‚îÄ logs                   # Audit (drift, fine-tuning, m√©triques)
```

---

## Cr√©ation des Datasets BigQuery

### 1. Via Console GCP

1. Aller sur [BigQuery Console](https://console.cloud.google.com/bigquery)
2. S√©lectionner le projet `datascientest-460618`
3. Cr√©er les 3 datasets :

#### Dataset 1 : `bike_traffic_raw`
```
Dataset ID: bike_traffic_raw
Location: europe-west1 (Belgium)
Default table expiration: Never
```

#### Dataset 2 : `bike_traffic_predictions`
```
Dataset ID: bike_traffic_predictions
Location: europe-west1 (Belgium)
Default table expiration: Never
```

#### Dataset 3 : `monitoring_audit`
```
Dataset ID: monitoring_audit
Location: europe-west1 (Belgium)
Default table expiration: Never
```

### 2. Via `bq` CLI

```bash
# Authentification
gcloud auth login
gcloud config set project datascientest-460618

# Cr√©er les datasets
bq mk --location=europe-west1 --dataset datascientest-460618:bike_traffic_raw
bq mk --location=europe-west1 --dataset datascientest-460618:bike_traffic_predictions
bq mk --location=europe-west1 --dataset datascientest-460618:monitoring_audit
```

### 3. Via Python (automatique dans DAGs)

Les DAGs cr√©eront automatiquement les datasets et tables si n√©cessaires gr√¢ce √† la fonction :

```python
from utils.bike_helpers import create_bq_dataset_if_not_exists, create_monitoring_table_if_needed

# Cr√©er les datasets
create_bq_dataset_if_not_exists("datascientest-460618", "bike_traffic_raw")
create_bq_dataset_if_not_exists("datascientest-460618", "bike_traffic_predictions")

# Cr√©er la table de monitoring avec son sch√©ma
create_monitoring_table_if_needed("datascientest-460618")
```

---

## Configuration Secret Manager

### Secrets requis pour PROD

| Secret ID | Description | Exemple de valeur |
|-----------|-------------|-------------------|
| `gcs-bucket-bike` | Nom du bucket GCS | `df_traffic_cyclist1` |
| `bq-project-bike` | ID du projet BigQuery | `datascientest-460618` |
| `bq-raw-dataset-bike` | Dataset donn√©es brutes | `bike_traffic_raw` |
| `bq-predict-dataset-bike` | Dataset pr√©dictions | `bike_traffic_predictions` |
| `bq-location` | Location BigQuery | `europe-west1` |
| `prod-bike-api-url` | URL de l'API RegModel | `https://regmodel-api-467498471756.europe-west1.run.app` |
| `bike-api-key-secret` | API Key pour s√©curit√© | `super-secret-prod-key-2024` |

### Cr√©ation des secrets via Console GCP

1. Aller sur [Secret Manager](https://console.cloud.google.com/security/secret-manager)
2. Cliquer sur **"CREATE SECRET"**
3. Pour chaque secret :
   - **Name** : utiliser exactement le `Secret ID` du tableau ci-dessus
   - **Secret value** : entrer la valeur correspondante
   - **Regions** : Automatic
   - Cliquer sur **CREATE**

### Cr√©ation des secrets via `gcloud` CLI

```bash
# Activer Secret Manager API
gcloud services enable secretmanager.googleapis.com --project=datascientest-460618

# Cr√©er les secrets
echo -n "df_traffic_cyclist1" | gcloud secrets create gcs-bucket-bike \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

echo -n "datascientest-460618" | gcloud secrets create bq-project-bike \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

echo -n "bike_traffic_raw" | gcloud secrets create bq-raw-dataset-bike \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

echo -n "bike_traffic_predictions" | gcloud secrets create bq-predict-dataset-bike \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

echo -n "europe-west1" | gcloud secrets create bq-location \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

echo -n "https://regmodel-api-467498471756.europe-west1.run.app" | gcloud secrets create prod-bike-api-url \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618

# G√©n√©rer et cr√©er l'API key (remplacer par une vraie cl√© forte)
echo -n "$(openssl rand -base64 32)" | gcloud secrets create bike-api-key-secret \
  --data-file=- \
  --replication-policy="automatic" \
  --project=datascientest-460618
```

### Donner acc√®s aux secrets pour le Service Account

```bash
# Service Account utilis√© par Cloud Run / Airflow
SERVICE_ACCOUNT="467498471756-compute@developer.gserviceaccount.com"

# Pour chaque secret, donner acc√®s en lecture
for SECRET_ID in gcs-bucket-bike bq-project-bike bq-raw-dataset-bike bq-predict-dataset-bike bq-location prod-bike-api-url bike-api-key-secret
do
  gcloud secrets add-iam-policy-binding $SECRET_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="roles/secretmanager.secretAccessor" \
    --project=datascientest-460618
done
```

---

## Variables d'environnement locales (DEV)

### Fichier `.env.airflow` (pour Docker Compose Airflow)

Cr√©er le fichier `.env.airflow` √† la racine du projet :

```bash
# Environment
ENV=DEV
GOOGLE_CLOUD_PROJECT=datascientest-460618

# BigQuery
BQ_PROJECT=datascientest-460618
BQ_RAW_DATASET=bike_traffic_raw
BQ_PREDICT_DATASET=bike_traffic_predictions
BQ_LOCATION=europe-west1

# GCS
GCS_BUCKET=df_traffic_cyclist1

# API
API_URL_DEV=http://regmodel-backend:8000
API_KEY_SECRET=dev-key-unsafe

# Google credentials
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp.json
```

### Fichier `.env` (pour RegModel backend)

V√©rifier que `backend/regmodel/.env` contient :

```bash
ENV=DEV
API_KEY_SECRET=dev-key-unsafe
GOOGLE_APPLICATION_CREDENTIALS=/app/gcp.json
```

---

## Validation de la configuration

### Test 1 : V√©rifier les secrets (PROD)

```python
# test_secrets.py
from google.cloud import secretmanager

def test_secrets():
    project_id = "datascientest-460618"
    client = secretmanager.SecretManagerServiceClient()

    secrets = [
        "gcs-bucket-bike",
        "bq-project-bike",
        "bq-raw-dataset-bike",
        "bq-predict-dataset-bike",
        "bq-location",
        "prod-bike-api-url",
        "bike-api-key-secret"
    ]

    for secret_id in secrets:
        try:
            name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
            response = client.access_secret_version(request={"name": name})
            value = response.payload.data.decode("UTF-8")
            print(f"‚úÖ {secret_id}: {value[:20]}...")
        except Exception as e:
            print(f"‚ùå {secret_id}: {e}")

if __name__ == "__main__":
    test_secrets()
```

### Test 2 : V√©rifier les datasets BigQuery

```bash
# Liste les datasets
bq ls --project_id=datascientest-460618

# Expected output:
#   bike_traffic_raw
#   bike_traffic_predictions
#   monitoring_audit
```

### Test 3 : V√©rifier la config Airflow (DEV)

```python
# Depuis un DAG ou un script
from utils.env_config import get_env_config

config = get_env_config()
print("üîß Configuration DEV:")
for key, value in config.items():
    print(f"  {key}: {value}")
```

### Test 4 : Cr√©er une table de test

```python
from google.cloud import bigquery
import pandas as pd
from datetime import datetime

# Client BigQuery
client = bigquery.Client(project="datascientest-460618")

# Test data
df = pd.DataFrame({
    "timestamp": [datetime.utcnow()],
    "test_value": [42]
})

# Write to BigQuery
table_id = "datascientest-460618.bike_traffic_raw.test_table"
df.to_gbq(
    destination_table=table_id,
    project_id="datascientest-460618",
    if_exists="replace",
    location="europe-west1"
)

print(f"‚úÖ Test table created: {table_id}")

# Clean up
client.delete_table(table_id)
print(f"üóëÔ∏è Test table deleted")
```

---

## Sch√©mas des tables

### Table `bike_traffic_raw.daily_YYYYMMDD`

```sql
CREATE TABLE `datascientest-460618.bike_traffic_raw.daily_20251011` (
  comptage_horaire INT64,
  date_et_heure_de_comptage TIMESTAMP,
  identifiant_du_compteur STRING,
  nom_du_compteur STRING,
  coordonnees_geographiques STRING,
  ingestion_ts TIMESTAMP
);
```

### Table `bike_traffic_predictions.daily_YYYYMMDD`

```sql
CREATE TABLE `datascientest-460618.bike_traffic_predictions.daily_20251011` (
  comptage_horaire INT64,
  prediction FLOAT64,
  model_type STRING,
  model_version STRING,
  prediction_ts TIMESTAMP,
  identifiant_du_compteur STRING
);
```

### Table `monitoring_audit.logs`

```sql
CREATE TABLE `datascientest-460618.monitoring_audit.logs` (
  timestamp TIMESTAMP,
  drift_detected BOOL,
  rmse FLOAT64,
  r2 FLOAT64,
  fine_tune_triggered BOOL,
  fine_tune_success BOOL,
  model_improvement FLOAT64,
  env STRING,
  error_message STRING
);
```

---

## Troubleshooting

### Erreur : "Permission denied" sur Secret Manager

**Solution** : V√©rifier les IAM permissions du Service Account :

```bash
gcloud projects add-iam-policy-binding datascientest-460618 \
  --member="serviceAccount:467498471756-compute@developer.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

### Erreur : "Dataset not found"

**Solution** : Cr√©er le dataset manuellement ou laisser le DAG le cr√©er automatiquement au premier run.

### Erreur : "Invalid credentials" dans Airflow

**Solution** : V√©rifier que `gcp.json` est bien mont√© dans le container Airflow :

```yaml
# docker-compose.yaml
volumes:
  - ./gcp.json:/opt/airflow/gcp.json:ro
```

---

## Checklist finale

- [ ] 3 datasets BigQuery cr√©√©s (`bike_traffic_raw`, `bike_traffic_predictions`, `monitoring_audit`)
- [ ] 7 secrets cr√©√©s dans Secret Manager
- [ ] Service Account a acc√®s aux secrets (r√¥le `secretmanager.secretAccessor`)
- [ ] Fichier `.env.airflow` cr√©√© avec variables DEV
- [ ] Fichier `gcp.json` pr√©sent et mont√© dans Docker
- [ ] Test de connexion BigQuery r√©ussi
- [ ] Test de lecture Secret Manager r√©ussi

---

**Pr√™t pour le d√©ploiement des DAGs Airflow !** üöÄ
