# Audit Urgent: Double Evaluation Metrics

**Date**: 2025-11-04
**Status**: ðŸ”´ CRITICAL - MÃ©triques manquantes
**Impact**: Monitoring incomplet, comparaisons trompeuses

---

## ðŸŽ¯ Objectif de l'Audit

VÃ©rifier que les **4 mÃ©triques RÂ² de la double Ã©valuation** sont correctement
exposÃ©es et monitorÃ©es:

```text
Champion (modÃ¨le en prod):
â”œâ”€ r2_champion_baseline   (test_baseline: 181K samples fixes)
â””â”€ r2_champion_current    (test_current: 20% fresh data)

Challenger (modÃ¨le retrained):
â”œâ”€ r2_challenger_baseline (test_baseline: detect regression)
â””â”€ r2_challenger_current  (test_current: measure improvement)
```

---

## âœ… Ce qui FONCTIONNE

### 1. DAG `monitor_and_fine_tune` - XCom Push

**Fichier**: [dags/dag_monitor_and_train.py:574-593](dags/dag_monitor_and_train.py#L574-L593)

```python
# âœ… Challenger metrics
context["ti"].xcom_push(key="r2_baseline", value=float(r2_baseline))  # Challenger baseline
context["ti"].xcom_push(key="r2_current", value=float(r2_current))    # Challenger current

# âœ… Champion baseline
context["ti"].xcom_push(key="champion_r2_baseline", value=float(champion_r2_baseline))

# âœ… Champion current (via validate_model task)
context["ti"].xcom_push(key="r2", value=float(r2))  # Ligne 200
```

**Verdict**: âœ… Les 4 mÃ©triques sont calculÃ©es et pushÃ©es vers XCom

---

### 2. BigQuery Audit Logs - Schema

**Fichier**: [monitoring/custom_exporters/airflow_exporter.py:237-260](monitoring/custom_exporters/airflow_exporter.py#L237-L260)

```sql
SELECT
    r2,              -- Champion (validation)
    r2_baseline,     -- Challenger baseline
    r2_current,      -- Challenger current
    ...
FROM `monitoring_audit.logs`
```

**Verdict**: âš ï¸ Partial - Manque `champion_r2_current` dans le schema BigQuery

---

## ðŸ”´ PROBLÃˆMES CRITIQUES

### ProblÃ¨me 1: Seulement 2 mÃ©triques Prometheus exposÃ©es

**Fichier**: [monitoring/custom_exporters/airflow_exporter.py:82-101](monitoring/custom_exporters/airflow_exporter.py#L82-L101)

**MÃ©triques actuelles**:

```python
BIKE_PREDICTION_R2 = Gauge("bike_prediction_r2")           # Challenger current âœ…
BIKE_MODEL_R2_PRODUCTION = Gauge("bike_model_r2_production")  # Champion baseline âœ…
```

**MÃ©triques MANQUANTES**:

```python
# âŒ Pas de mÃ©trique pour Champion current
# âŒ Pas de mÃ©trique pour Challenger baseline
```

**Impact**:

- Impossible de comparer les 2 modÃ¨les sur le **mÃªme test set**
- Dashboards montrent "apples vs oranges" (baseline vs current)
- DÃ©cisions basÃ©es sur des comparaisons non Ã©quitables

---

### ProblÃ¨me 2: Logique d'exposition confuse

**Fichier**: [monitoring/custom_exporters/airflow_exporter.py:436-466](monitoring/custom_exporters/airflow_exporter.py#L436-L466)

```python
if deployment_decision == "deploy":
    r2 = bq_metrics.get("r2_current")  # Challenger current
    BIKE_MODEL_R2_PRODUCTION.set(r2_float)  # âš ï¸ Ã‰crase champion!
else:
    r2 = bq_metrics.get("r2")  # Champion
    BIKE_MODEL_R2_PRODUCTION.set(r2_float)
```

**ProblÃ¨me**: `bike_model_r2_production` change de signification selon le dÃ©ploiement!

---

### ProblÃ¨me 3: Dashboards trompeurs

**Fichier**: `monitoring/grafana/provisioning/dashboards/model_performance.json`

**Query actuelle**:

```promql
bike_model_r2_production   # Champion baseline (0.867)
bike_prediction_r2         # Challenger current (0.528)
```

**LÃ©gende affichÃ©e**: "Champion vs Challenger"
**RÃ©alitÃ©**: "Champion sur baseline vs Challenger sur current" â†’ **Comparaison invalide!**

---

## ðŸ“Š DonnÃ©es Manquantes

### Ce que nous AVONS

| MÃ©trique | Source | Valeur |
|----------|--------|--------|
| Champion baseline | XCom `champion_r2_baseline` | 0.867 âœ… |
| Champion current | XCom `r2` (validate_model) | ??? âš ï¸ |
| Challenger baseline | XCom `r2_baseline` | ??? âŒ |
| Challenger current | XCom `r2_current` | 0.528 âœ… |

### Ce que nous EXPOSONS Ã  Prometheus

| MÃ©trique Prometheus | Correspond Ã  | ExposÃ©e? |
|---------------------|--------------|----------|
| `bike_model_r2_production` | Champion baseline | âœ… |
| `bike_prediction_r2` | Challenger current | âœ… |
| `bike_model_r2_champion_current` | Champion current | âŒ |
| `bike_model_r2_challenger_baseline` | Challenger baseline | âŒ |

---

## ðŸŽ¯ Actions Correctives Requises

### Action 1: Ajouter les 2 mÃ©triques manquantes dans Airflow Exporter

**Fichier**: `monitoring/custom_exporters/airflow_exporter.py`

**Ajouter aprÃ¨s ligne 101**:

```python
# Double evaluation - Full metrics
BIKE_MODEL_R2_CHAMPION_BASELINE = Gauge(
    "bike_model_r2_champion_baseline",
    "Champion model RÂ² on test_baseline (fixed reference)",
)
BIKE_MODEL_R2_CHAMPION_CURRENT = Gauge(
    "bike_model_r2_champion_current",
    "Champion model RÂ² on test_current (new distribution)",
)
BIKE_MODEL_R2_CHALLENGER_BASELINE = Gauge(
    "bike_model_r2_challenger_baseline",
    "Challenger model RÂ² on test_baseline (regression check)",
)
BIKE_MODEL_R2_CHALLENGER_CURRENT = Gauge(
    "bike_model_r2_challenger_current",
    "Challenger model RÂ² on test_current (improvement check)",
)
```

---

### Action 2: Modifier la logique de collection BigQuery

**Fichier**: `monitoring/custom_exporters/airflow_exporter.py:416-486`

**Remplacer la logique actuelle par**:

```python
def _collect_monitoring_metrics(self, dag_id: str, dag_run_id: str) -> None:
    # Try BigQuery first
    bq_metrics = self._get_latest_monitoring_metrics_from_bq()

    if bq_metrics:
        # Champion metrics (from validate_model task)
        champion_r2 = bq_metrics.get("r2")  # Champion current (validation)
        champion_r2_baseline = None  # TODO: Add to BigQuery schema

        # Challenger metrics (from training)
        challenger_r2_baseline = bq_metrics.get("r2_baseline")
        challenger_r2_current = bq_metrics.get("r2_current")

        # Set all 4 metrics
        if champion_r2 is not None:
            BIKE_MODEL_R2_CHAMPION_CURRENT.set(float(champion_r2))

        if champion_r2_baseline is not None:
            BIKE_MODEL_R2_CHAMPION_BASELINE.set(float(champion_r2_baseline))

        if challenger_r2_baseline is not None:
            BIKE_MODEL_R2_CHALLENGER_BASELINE.set(float(challenger_r2_baseline))

        if challenger_r2_current is not None:
            BIKE_MODEL_R2_CHALLENGER_CURRENT.set(float(challenger_r2_current))

        # Legacy metrics (keep for backward compatibility)
        BIKE_MODEL_R2_PRODUCTION.set(float(champion_r2))  # Always champion
        BIKE_PREDICTION_R2.set(float(challenger_r2_current))
```

---

### Action 3: Ajouter `champion_r2_current` au schema BigQuery

**Fichier**: `dags/dag_monitor_and_train.py:705-731`

**Ajouter le champ manquant**:

```python
audit_record = {
    # ...existing fields...
    "r2": float(r2) if r2 else 0.0,  # Champion current (validation)
    "r2_baseline": float(r2_baseline) if r2_baseline is not None else None,  # Challenger baseline
    "r2_current": float(r2_current) if r2_current is not None else None,  # Challenger current
    # NEW: Add champion evaluated on current
    "champion_r2_current": float(champion_r2_current) if champion_r2_current else None,
    "champion_r2_baseline": float(champion_r2_baseline) if champion_r2_baseline else None,
}
```

**Note**: Requiert `ALTER TABLE` sur BigQuery ou recrÃ©ation de la table.

---

### Action 4: Mettre Ã  jour les dashboards Grafana

**Fichier**: `monitoring/grafana/provisioning/dashboards/model_performance.json`

**Nouveau panel: "Fair Comparison - Both Models on test_baseline"**:

```json
{
  "title": "RÂ² Comparison - test_baseline (Fair)",
  "targets": [
    {
      "expr": "bike_model_r2_champion_baseline",
      "legendFormat": "Champion (baseline)"
    },
    {
      "expr": "bike_model_r2_challenger_baseline",
      "legendFormat": "Challenger (baseline)"
    }
  ]
}
```

**Nouveau panel: "Fair Comparison - Both Models on test_current"**:

```json
{
  "title": "RÂ² Comparison - test_current (Fair)",
  "targets": [
    {
      "expr": "bike_model_r2_champion_current",
      "legendFormat": "Champion (current)"
    },
    {
      "expr": "bike_model_r2_challenger_current",
      "legendFormat": "Challenger (current)"
    }
  ]
}
```

---

### Action 5: Mettre Ã  jour les alertes Grafana

**Fichier**: `monitoring/grafana/provisioning/alerting/rules.yml`

**ProblÃ¨me**: Les alertes utilisent `bike_model_r2_production` qui est ambigu.

**Solution**: CrÃ©er des alertes spÃ©cifiques:

```yaml
- name: model_performance_champion_baseline
  condition: bike_model_r2_champion_baseline < 0.65

- name: model_performance_champion_current
  condition: bike_model_r2_champion_current < 0.65

- name: challenger_regression_detected
  condition: bike_model_r2_challenger_baseline < 0.60
```

---

## ðŸ“ˆ BÃ©nÃ©fices Attendus

### Avant (Ã©tat actuel)

```text
Grafana Dashboard:
â”œâ”€ Champion baseline: 0.867 âœ…
â””â”€ Challenger current: 0.528 âŒ

âŒ Comparaison invalide (test sets diffÃ©rents)
âŒ Impossible de savoir si challenger amÃ©liore rÃ©ellement
âŒ DÃ©cisions basÃ©es sur des mÃ©triques trompeuses
```

### AprÃ¨s (Ã©tat corrigÃ©)

```text
Grafana Dashboard:
â”œâ”€ Panel 1: Both models on test_baseline (fair comparison)
â”‚   â”œâ”€ Champion baseline: 0.867
â”‚   â””â”€ Challenger baseline: 0.60 â†’ REJECT (rÃ©gression!)
â”‚
â””â”€ Panel 2: Both models on test_current (fair comparison)
    â”œâ”€ Champion current: 0.75
    â””â”€ Challenger current: 0.78 â†’ DEPLOY (amÃ©lioration!)

âœ… Comparaisons Ã©quitables
âœ… DÃ©cisions basÃ©es sur mÃ©triques correctes
âœ… Monitoring complet de la double Ã©valuation
```

---

## ðŸ” VÃ©rifications Post-Correction

### 1. VÃ©rifier les mÃ©triques Prometheus

```bash
# Check all 4 metrics are exposed
curl http://localhost:9101/metrics | grep "bike_model_r2"

# Expected output (4 metrics):
# bike_model_r2_champion_baseline 0.867
# bike_model_r2_champion_current 0.75
# bike_model_r2_challenger_baseline 0.60
# bike_model_r2_challenger_current 0.78
```

### 2. VÃ©rifier BigQuery audit logs

```sql
SELECT
    timestamp,
    r2,                      -- Champion current
    champion_r2_baseline,    -- Champion baseline (NEW)
    champion_r2_current,     -- Champion current (duplicate of r2)
    r2_baseline,             -- Challenger baseline
    r2_current,              -- Challenger current
    deployment_decision
FROM `monitoring_audit.logs`
ORDER BY timestamp DESC
LIMIT 5;
```

### 3. VÃ©rifier Grafana dashboards

- Dashboard "Model Performance" doit montrer 2 nouveaux panels
- LÃ©gendes doivent indiquer explicitement le test set utilisÃ©
- Comparaisons doivent Ãªtre sur le **mÃªme test set**

---

## â±ï¸ PrioritÃ© d'ImplÃ©mentation

| Action | PrioritÃ© | Effort | Impact |
|--------|----------|--------|--------|
| Action 1: Ajouter mÃ©triques Prometheus | ðŸ”´ HIGH | 30 min | Critique |
| Action 2: Modifier collection logic | ðŸ”´ HIGH | 1h | Critique |
| Action 3: Schema BigQuery | ðŸŸ¡ MEDIUM | 30 min | Important |
| Action 4: Dashboards Grafana | ðŸŸ¡ MEDIUM | 1h | Important |
| Action 5: Alertes Grafana | ðŸŸ¢ LOW | 30 min | Nice-to-have |

**Total effort estimÃ©**: 3-4 heures

---

## ðŸ“š RÃ©fÃ©rences

- [dags/dag_monitor_and_train.py](dags/dag_monitor_and_train.py) - Ligne 365-598
- [monitoring/custom_exporters/airflow_exporter.py](monitoring/custom_exporters/airflow_exporter.py) - Ligne 82-536
- [docs/sliding_window.md](docs/sliding_window.md) - Double evaluation strategy
- [docs/training_strategy.md](docs/training_strategy.md) - Decision logic

---

**Conclusion**: Le systÃ¨me calcule correctement les 4 mÃ©triques mais
**n'en expose que 2**. Les dashboards sont **trompeurs** car ils comparent
des modÃ¨les sur des test sets diffÃ©rents. Les actions correctives sont
**critiques** pour un monitoring MLOps fiable.
