"""
Helper functions for bike traffic DAGs
Provides utilities for BigQuery, GCS, and data processing
"""

import os
import time
from datetime import datetime, timedelta

import gcsfs
import pandas as pd
from google.cloud import bigquery


def get_storage_path(subdir: str, filename: str) -> str:
    """
    Returns the environment-aware storage path for a given subdir and filename.
    DEV: Local filesystem under ./data/
    PROD: Google Cloud Storage bucket path

    Args:
        subdir: Subdirectory within storage root (e.g., 'raw_data', 'models')
        filename: File name (can be empty for directory paths)

    Returns:
        Full path string (gs://... or local path)

    Examples:
        DEV:  get_storage_path("raw_data", "current.csv") â†’ "./data/current.csv"
        PROD: get_storage_path("raw_data", "current.csv") â†’ "gs://df_traffic_cyclist1/raw_data/current.csv"
    """
    env = os.getenv("ENV", "DEV")
    gcs_bucket = os.getenv("GCS_BUCKET", "df_traffic_cyclist1")

    if env == "PROD":
        # Use GCS path (bucket structure: raw_data/, models/, mlruns/, dvc-storage/)
        if subdir:
            return (
                f"gs://{gcs_bucket}/{subdir}/{filename}"
                if filename
                else f"gs://{gcs_bucket}/{subdir}/"
            )
        else:
            return (
                f"gs://{gcs_bucket}/{filename}" if filename else f"gs://{gcs_bucket}/"
            )
    else:
        # Use local path (project structure: data/, models/, mlruns/, etc.)
        # Special mapping: raw_data â†’ data/ localement
        local_subdir = "data" if subdir == "raw_data" else subdir

        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
        if local_subdir:
            full_path = (
                os.path.join(base_dir, local_subdir, filename)
                if filename
                else os.path.join(base_dir, local_subdir) + "/"
            )
        else:
            full_path = os.path.join(base_dir, filename) if filename else base_dir + "/"

        return full_path


def get_reference_data_path() -> str:
    """
    Returns the path to reference_data.csv
    DEV: ./data/reference_data.csv
    PROD: gs://df_traffic_cyclist1/raw_data/reference_data.csv
    """
    env = os.getenv("ENV", "DEV")
    if env == "PROD":
        bucket = os.getenv("GCS_BUCKET", "df_traffic_cyclist1")
        return f"gs://{bucket}/raw_data/reference_data.csv"
    else:
        # Local: data/reference_data.csv
        return os.path.abspath(
            os.path.join(os.path.dirname(__file__), "../../data/reference_data.csv")
        )


def get_current_data_path() -> str:
    """
    Returns the path to current_data.csv
    DEV: ./data/current_data.csv
    PROD: gs://df_traffic_cyclist1/raw_data/current_data.csv
    """
    env = os.getenv("ENV", "DEV")
    if env == "PROD":
        bucket = os.getenv("GCS_BUCKET", "df_traffic_cyclist1")
        return f"gs://{bucket}/raw_data/current_data.csv"
    else:
        # Local: data/current_data.csv
        return os.path.abspath(
            os.path.join(os.path.dirname(__file__), "../../data/current_data.csv")
        )


def read_gcs_csv(path: str) -> pd.DataFrame:
    """
    Lit un fichier CSV, que ce soit en local ou sur GCS (gs://...).

    Args:
        path: Le chemin vers le fichier CSV

    Returns:
        DataFrame chargÃ©

    Raises:
        FileNotFoundError: Si le fichier est introuvable
    """
    if path.startswith("gs://"):
        fs = gcsfs.GCSFileSystem(skip_instance_cache=True, cache_timeout=0)
        if not fs.exists(path):
            raise FileNotFoundError(f"â›” Fichier introuvable sur GCS: {path}")
        with fs.open(path, "r") as f:
            return pd.read_csv(f)
    else:
        if not os.path.exists(path):
            raise FileNotFoundError(f"â›” Fichier local introuvable: {path}")
        return pd.read_csv(path)


def write_csv(df: pd.DataFrame, path: str):
    """
    Ã‰crit un DataFrame en CSV (local ou GCS)

    Args:
        df: DataFrame Ã  Ã©crire
        path: Chemin de destination (local ou gs://)
    """
    if path.startswith("gs://"):
        print(f"ğŸ“ Saving to GCS: {path}")
        fs = gcsfs.GCSFileSystem(skip_instance_cache=True, cache_timeout=0)
        with fs.open(path, "w") as f:
            df.to_csv(f, index=False)
            f.flush()
        fs.invalidate_cache(path)
        # Validation immÃ©diate
        if not fs.exists(path):
            raise RuntimeError(f"âŒ GCS file not found right after saving: {path}")
        print(f"âœ… File written and verified on GCS: {path}")
    else:
        print(f"ğŸ“ Saving locally: {path}")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        df.to_csv(path, index=False)


def file_exists(path: str) -> bool:
    """VÃ©rifie si un fichier existe (local ou GCS)"""
    if path.startswith("gs://"):
        fs = gcsfs.GCSFileSystem(skip_instance_cache=True, cache_timeout=0)
        return fs.exists(path)
    else:
        return os.path.exists(path)


def wait_for_gcs(path: str, timeout: int = 30):
    """
    Attends que le fichier GCS soit visible (avec un timeout en secondes)

    Args:
        path: Chemin GCS Ã  vÃ©rifier
        timeout: Temps maximum d'attente en secondes

    Raises:
        FileNotFoundError: Si le fichier n'apparaÃ®t pas aprÃ¨s timeout
    """
    if not path.startswith("gs://"):
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ Local file not found: {path}")
        return

    fs = gcsfs.GCSFileSystem(skip_instance_cache=True, cache_timeout=0)
    for i in range(timeout):
        if fs.exists(path):
            print(f"âœ… GCS file detected: {path}")
            return
        print(f"â³ Waiting for GCS propagation ({i+1}/{timeout}): {path}")
        time.sleep(1)

    raise FileNotFoundError(f"â›” File not found in GCS after {timeout}s: {path}")


def create_bq_dataset_if_not_exists(
    project_id: str, dataset_id: str, location: str = "europe-west1"
):
    """
    CrÃ©e un dataset BigQuery s'il n'existe pas dÃ©jÃ 

    Args:
        project_id: ID du projet GCP
        dataset_id: ID du dataset Ã  crÃ©er
        location: Location du dataset (dÃ©faut: europe-west1)
    """
    client = bigquery.Client(project=project_id)
    dataset_ref = f"{project_id}.{dataset_id}"

    try:
        client.get_dataset(dataset_ref)
        print(f"âœ… Dataset exists: {dataset_ref}")
    except Exception as e:
        if "Not found" in str(e) or "404" in str(e):
            print(f"âš ï¸ Dataset not found. Creating: {dataset_ref}")
            dataset = bigquery.Dataset(dataset_ref)
            dataset.location = location
            try:
                client.create_dataset(dataset, exists_ok=True)
                print(f"âœ… Dataset created: {dataset_ref}")
            except Exception as create_error:
                if "Already Exists" in str(create_error):
                    print(f"âœ… Dataset already exists (race condition): {dataset_ref}")
                else:
                    raise
        else:
            raise


def create_monitoring_table_if_needed(
    project_id: str,
    dataset_id: str = "monitoring_audit",
    table_id: str = "logs",
    location: str = "europe-west1",
):
    """
    CrÃ©e la table de monitoring si elle n'existe pas

    Args:
        project_id: ID du projet GCP
        dataset_id: ID du dataset (dÃ©faut: monitoring_audit)
        table_id: ID de la table (dÃ©faut: logs)
        location: Location (dÃ©faut: europe-west1)
    """
    client = bigquery.Client(project=project_id)
    full_dataset_id = f"{project_id}.{dataset_id}"
    full_table_id = f"{full_dataset_id}.{table_id}"

    # VÃ©rifie si le dataset existe
    try:
        client.get_dataset(full_dataset_id)
        print(f"âœ… Dataset exists: {full_dataset_id}")
    except Exception as e:
        if "Not found" in str(e) or "404" in str(e):
            print(f"âš ï¸ Dataset not found. Creating: {full_dataset_id}")
            dataset = bigquery.Dataset(full_dataset_id)
            dataset.location = location
            try:
                client.create_dataset(dataset, exists_ok=True)
                print(f"âœ… Dataset created: {full_dataset_id}")
            except Exception as create_error:
                if "Already Exists" in str(create_error):
                    print(
                        f"âœ… Dataset already exists (race condition): {full_dataset_id}"
                    )
                else:
                    raise
        else:
            raise

    # VÃ©rifie si la table existe
    try:
        client.get_table(full_table_id)
        print(f"âœ… Table already exists: {full_table_id}")
    except Exception:
        print(f"âš ï¸ Table not found, creating: {full_table_id}")
        schema = [
            bigquery.SchemaField("timestamp", "TIMESTAMP"),
            bigquery.SchemaField("drift_detected", "BOOL"),
            bigquery.SchemaField("rmse", "FLOAT"),
            bigquery.SchemaField("r2", "FLOAT"),
            bigquery.SchemaField("fine_tune_triggered", "BOOL"),
            bigquery.SchemaField("fine_tune_success", "BOOL"),
            bigquery.SchemaField("model_improvement", "FLOAT"),
            bigquery.SchemaField("env", "STRING"),
            bigquery.SchemaField("error_message", "STRING"),
        ]
        table = bigquery.Table(full_table_id, schema=schema)
        client.create_table(table)
        print(f"âœ… Table created: {full_table_id}")


def fetch_historical_data_from_bq(
    bq_client: bigquery.Client,
    bq_project: str,
    dataset: str,
    days_back: int = 7,
    limit_per_day: int = 100,
    verbose: bool = True,
) -> pd.DataFrame:
    """
    Fetches historical data from the past N days from BigQuery.

    Args:
        bq_client: Initialized BigQuery client
        bq_project: Project ID
        dataset: Dataset name (e.g. 'bike_traffic_raw')
        days_back: Number of past days to search
        limit_per_day: Maximum records per day
        verbose: Whether to print progress logs

    Returns:
        A DataFrame of historical samples (may be empty)
    """
    data_frames = []

    for i in range(1, days_back + 1):
        day = (datetime.utcnow() - timedelta(days=i)).strftime("%Y%m%d")
        table_id = f"{bq_project}.{dataset}.daily_{day}"

        try:
            query = f"SELECT * FROM `{table_id}` LIMIT {limit_per_day}"  # nosec B608
            df = bq_client.query(query).to_dataframe()

            if not df.empty:
                data_frames.append(df)
                if verbose:
                    print(f"âœ… Found {len(df)} records in {table_id}")

        except Exception as e:
            if verbose:
                print(f"âš ï¸ Skipped {table_id}: {e}")

    if data_frames:
        return pd.concat(data_frames, ignore_index=True)
    else:
        if verbose:
            print("ğŸš« No historical data found")
        return pd.DataFrame()


def host_to_docker_path(path: str) -> str:
    """
    Convertit un chemin absolu local (host) en chemin Docker /app/...
    Utile pour passer des paths entre Airflow (host) et API (Docker)

    Args:
        path: Chemin local absolu

    Returns:
        Chemin Docker Ã©quivalent
    """
    # Replace host base path with Docker /app path
    base_host_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
    if path.startswith(base_host_path):
        relative_path = os.path.relpath(path, base_host_path)
        return f"/app/{relative_path}"
    return path


if __name__ == "__main__":
    # Test des fonctions
    print("ğŸ§ª Testing bike_helpers functions...")

    # Test get_storage_path
    print("\nğŸ“ Testing get_storage_path:")
    print(f"  DEV raw_data: {get_storage_path('raw_data', 'test.csv')}")
    print(f"  DEV models: {get_storage_path('models', 'model.pkl')}")

    # Test avec ENV=PROD
    os.environ["ENV"] = "PROD"
    print(f"  PROD raw_data: {get_storage_path('raw_data', 'test.csv')}")
    print(f"  PROD models: {get_storage_path('models', 'model.pkl')}")

    # Test reference/current paths
    os.environ["ENV"] = "DEV"
    print(f"\nğŸ“Š Reference data path (DEV): {get_reference_data_path()}")
    print(f"ğŸ“Š Current data path (DEV): {get_current_data_path()}")

    print("\nâœ… Tests completed!")
