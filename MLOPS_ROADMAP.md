# üéØ MLOps Roadmap ‚Äî Bike Traffic Prediction

**Date limite soutenance** : 7 novembre 2025
**Branche principale** : `feat/mlops-integration`

---

## üìä √âtat actuel (Phases 0-1 compl√©t√©es ‚úÖ)

- ‚úÖ Mod√®les ML entra√Æn√©s (RF, NN)
- ‚úÖ MLflow tracking op√©rationnel (dev/prod)
- ‚úÖ Registry custom via `summary.json` GCS
- ‚úÖ Backend FastAPI d√©ploy√© sur Cloud Run (regmodel uniquement)
- ‚úÖ Frontend Streamlit d√©ploy√©
- ‚úÖ Docker + docker-compose pour dev local
- ‚úÖ Environnements dev/prod s√©par√©s

---

## üöÄ Phases MLOps √† impl√©menter

### **Phase 2 : Tests, CI & Data Versioning**

#### **2.1 Data Versioning with DVC** (`feat/mlops-dvc-data-versioning`) ‚úÖ

**Implementation completed** ‚úÖ

üìö **Full documentation**: [docs/dvc.md](docs/dvc.md)

**Deliverables** ‚úÖ:

- ‚úÖ Temporal split: reference (660K rows, 69.7%) + current (288K rows, 30.3%)
- ‚úÖ DVC tracking with GCS remote storage
- ‚úÖ `scripts/split_data_temporal.py` implemented

---

#### **2.2 Tests unitaires + CI** (`feat/mlops-tests-ci`) ‚úÖ

**Implementation completed** ‚úÖ

üìö **Full documentation**:

- [docs/pytest.md](docs/pytest.md) - Complete test suite
- [docs/ci.md](docs/ci.md) - CI/CD with GitHub Actions + Codecov

**Deliverables** ‚úÖ:

- ‚úÖ **47 tests** passing (13 pipelines + 17 preprocessing + 11 API + 6 registry)
- ‚úÖ **68% coverage** (app/classes: 73.42%, model_registry: 56.31%)
- ‚úÖ GitHub Actions CI configured with **UV**
- ‚úÖ Codecov integration active ([dashboard](https://app.codecov.io/gh/arthurcornelio88/bike-count-prediction-app))
- ‚úÖ Coverage artifacts (HTML reports, 30 days retention)

**Files created**:

```text
tests/
‚îú‚îÄ‚îÄ test_pipelines.py          ‚úÖ 13 tests (RF, NN)
‚îú‚îÄ‚îÄ test_preprocessing.py      ‚úÖ 17 tests (transformers)
‚îú‚îÄ‚îÄ test_api_regmodel.py       ‚úÖ 11 tests (FastAPI /predict)
‚îú‚îÄ‚îÄ test_model_registry.py     ‚úÖ 6 tests (summary.json logic)
‚îú‚îÄ‚îÄ conftest.py                ‚úÖ Shared fixtures
pytest.ini                     ‚úÖ Configuration
.github/workflows/ci.yml       ‚úÖ GitHub Actions
.coveragerc                    ‚úÖ Coverage config
```

---

#### **2.3 Backend API `/train` + MLflow Integration** (`feat/mlops-tests-ci`) ‚úÖ

**Implementation completed** ‚úÖ

üìö **Documentation**: [docs/backend.md](docs/backend.md#train---train-and-upload-model)

**Objectifs** :

- ‚úÖ Refactor training logic into unified `train_model()` function
- ‚úÖ Create FastAPI `/train` endpoint for remote training
- ‚úÖ Integrate MLflow tracking in docker-compose stack
- ‚úÖ Support DVC-tracked datasets (reference/current)
- ‚úÖ Automatic GCS upload + `summary.json` update

**Deliverables** ‚úÖ:

- ‚úÖ `train_model()` function in [train.py:256](backend/regmodel/app/train.py#L256)
- ‚úÖ `/train` endpoint in [fastapi_app.py:101](backend/regmodel/app/fastapi_app.py#L101)
- ‚úÖ Docker Compose with RegModel API + MLflow server
- ‚úÖ UV-optimized Dockerfile ([backend/regmodel/Dockerfile](backend/regmodel/Dockerfile))
- ‚úÖ Dedicated pyproject.toml for RegModel service
- ‚úÖ MLflow tracking already integrated in `train_rf()`, `train_nn()`, `train_rfc()`

**Architecture**:

```yaml
services:
  mlflow:
    - Tracking server on port 5000
    - Backend store: ./mlruns_dev
    - Artifacts: ./mlflow_artifacts
    - Healthcheck enabled

  regmodel-backend:
    - FastAPI on port 8000
    - Depends on MLflow (healthcheck)
    - Mounts: code, GCS credentials, data
    - Hot reload enabled (dev mode)
```

**API Usage**:

```bash
# Train RF model on reference data
curl -X POST "http://localhost:8000/train" \
  -H "Content-Type: application/json" \
  -d '{
    "model_type": "rf",
    "data_source": "reference",
    "env": "prod"
  }'

# Response includes: run_id, metrics, model_uri
```

**Supported models**:

- `rf`: Random Forest regressor
- `nn`: Neural Network regressor
- `rf_class`: Random Forest classifier (affluence detection)

**M√©triques track√©es** (align√© avec `summary.json`) :

- **R√©gression (RF, NN)** : `r2_train`, `rmse_train`
- **Classification (RFC)** : `accuracy`, `precision`, `recall`, `f1_score`
- **Hyperparams** :
  - RF: `n_estimators`, `max_depth`, `random_state`
  - NN: `embedding_dim`, `batch_size`, `epochs`, `total_params`

**Validation completed** ‚úÖ:

- ‚úÖ Full stack tested: `docker compose up` works
- ‚úÖ MLflow UI accessible at <http://localhost:5000>
- ‚úÖ `/train` endpoint tested with RF, NN models
- ‚úÖ Test mode (`test_mode=true`) working with `test_sample.csv` (6s for NN, ~30s for RF)
- ‚úÖ Metrics correctly returned in API response (RMSE, R¬≤)
- ‚úÖ MLflow tracking confirmed (runs, metrics, tags, artifacts)

---

### **Phase 3 : Orchestration Airflow + Monitoring Production** (`feat/mlops-airflow-pipeline`)

**Objectifs unifi√©s** :

- üîÑ Pipeline automatis√© end-to-end avec Airflow
- üìä Monitoring avec BigQuery (raw, predictions, audit)
- üîç Drift detection avec Evidently
- üéØ R√©entra√Ænement intelligent via endpoint `/train` (fine-tuning)
- üìà M√©triques API avec Prometheus + Grafana
- üîí S√©curit√© API (API Key + Rate Limiting)

---

#### **3.1 Architecture BigQuery**

**3 Datasets pour tra√ßabilit√© compl√®te** :

```yaml
# Structure BigQuery
datascientest-460618:
  bike_traffic_raw:           # Donn√©es brutes quotidiennes
    - daily_YYYYMMDD          # Tables par jour (comptage horaire)

  bike_traffic_predictions:   # Pr√©dictions quotidiennes
    - daily_YYYYMMDD          # Pr√©dictions + scores de confiance
    - prediction_ts           # Timestamp de pr√©diction

  monitoring_audit:           # Logs de monitoring et r√©entra√Ænement
    - logs                    # Audit complet (drift, AUC, fine-tuning)
```

**Schema des tables** :

```python
# bike_traffic_raw.daily_YYYYMMDD
{
    "Comptage horaire": INTEGER,
    "Date et heure de comptage": TIMESTAMP,
    "Identifiant du compteur": STRING,
    "Nom du compteur": STRING,
    "Coordonn√©es g√©ographiques": STRING,
    "ingestion_ts": TIMESTAMP
}

# bike_traffic_predictions.daily_YYYYMMDD
{
    "Comptage horaire": INTEGER,          # Valeur r√©elle (si disponible)
    "prediction": FLOAT,                   # Pr√©diction du mod√®le
    "model_type": STRING,                  # rf, nn, rf_class
    "model_version": STRING,               # Timestamp du mod√®le
    "prediction_ts": TIMESTAMP
}

# monitoring_audit.logs
{
    "timestamp": TIMESTAMP,
    "drift_detected": BOOLEAN,
    "rmse": FLOAT,
    "r2": FLOAT,
    "fine_tune_triggered": BOOLEAN,
    "fine_tune_success": BOOLEAN,
    "model_improvement": FLOAT,            # Œî R¬≤
    "env": STRING,
    "error_message": STRING
}
```

---

#### **3.2 DAGs Airflow (Architecture modulaire)**

**3 DAGs s√©par√©s pour isoler les responsabilit√©s** :

```mermaid
graph LR
    A[dag_daily_fetch_data] -->|@daily| B[BigQuery raw]
    C[dag_daily_prediction] -->|@daily| D[BigQuery predictions]
    E[dag_monitor_and_train] -->|@weekly| F{Drift?}
    F -->|Yes| G[Evaluate Model]
    G -->|Poor R¬≤| H[Fine-tune via /train]
    G -->|Good R¬≤| I[End]
    H --> J[Update BigQuery audit]
```

**üìÅ Structure des fichiers** :

```text
dags/
‚îú‚îÄ‚îÄ dag_daily_fetch_data.py          # Ingestion donn√©es brutes ‚Üí BigQuery
‚îú‚îÄ‚îÄ dag_daily_prediction.py          # Pr√©dictions via /predict ‚Üí BigQuery
‚îú‚îÄ‚îÄ dag_monitor_and_train.py         # Drift + Eval + Fine-tuning
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ bike_helpers.py               # Fonctions BigQuery, GCS
    ‚îî‚îÄ‚îÄ env_config.py                 # Config ENV/PROD avec Secret Manager
```

---

#### **3.3 DAG 1 : Ingestion des donn√©es** (`dag_daily_fetch_data.py`)

**Objectif** : R√©cup√©rer les donn√©es de trafic cycliste et stocker dans BigQuery

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()  # G√®re DEV/PROD + Secret Manager

def fetch_bike_data_to_bq(**context):
    """
    Fetch latest bike traffic data from Paris Open Data API
    Store in BigQuery: bike_traffic_raw.daily_YYYYMMDD
    """
    today = datetime.utcnow().strftime("%Y%m%d")

    # Paris Open Data API (comptage v√©lo)
    api_url = "https://opendata.paris.fr/api/explore/v2.1/catalog/datasets/comptage-velo-donnees-compteurs/records"
    params = {
        "limit": 1000,
        "order_by": "date_et_heure_de_comptage DESC"
    }

    response = requests.get(api_url, params=params)
    if response.status_code != 200:
        raise Exception(f"‚ùå API failed: {response.status_code}")

    data = response.json()
    df = pd.DataFrame([r['fields'] for r in data['results']])
    df["ingestion_ts"] = datetime.utcnow().isoformat()

    # Write to BigQuery
    table_id = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df.to_gbq(
        destination_table=table_id,
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="replace",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"‚úÖ Ingested {len(df)} records into {table_id}")

with DAG(
    dag_id="daily_fetch_bike_data",
    schedule_interval="@daily",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "ingestion", "bigquery"]
) as dag:

    fetch_task = PythonOperator(
        task_id="fetch_to_bigquery",
        python_callable=fetch_bike_data_to_bq
    )
```

---

#### **3.4 DAG 2 : Pr√©dictions quotidiennes** (`dag_daily_prediction.py`)

**Objectif** : Lire BigQuery ‚Üí Pr√©dire via `/predict` ‚Üí Stocker r√©sultats

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()

def run_daily_prediction(**context):
    """
    1. Read from BigQuery raw table
    2. Call /predict endpoint
    3. Store predictions in BigQuery predictions table
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # 1Ô∏è‚É£ Read raw data
    raw_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df = bq.query(f"SELECT * FROM `{raw_table}` LIMIT 500").to_dataframe()

    # 2Ô∏è‚É£ Call /predict endpoint
    api_url = f"{ENV_CONFIG['API_URL']}/predict"
    response = requests.post(api_url, json={
        "records": df.to_dict(orient="records"),
        "model_type": "rf",
        "metric": "r2"
    })

    if response.status_code != 200:
        raise Exception(f"‚ùå Prediction failed: {response.text}")

    predictions = response.json()["predictions"]
    df["prediction"] = predictions
    df["model_type"] = "rf"
    df["prediction_ts"] = datetime.utcnow().isoformat()

    # 3Ô∏è‚É£ Store in BigQuery
    pred_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_predictions.daily_{today}"
    df.to_gbq(
        destination_table=pred_table,
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="replace",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"‚úÖ Predictions saved to {pred_table}")

with DAG(
    dag_id="daily_prediction",
    schedule_interval="@daily",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "prediction", "bigquery"]
) as dag:

    predict_task = PythonOperator(
        task_id="predict_daily_data",
        python_callable=run_daily_prediction
    )
```

---

#### **3.5 DAG 3 : Monitoring + Fine-tuning** (`dag_monitor_and_train.py`)

**Objectif** : Drift detection ‚Üí Validation ‚Üí Fine-tuning conditionnel

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()

# 1Ô∏è‚É£ DRIFT DETECTION
def run_drift_monitoring(**context):
    """
    Compare reference vs current data using Evidently
    Calls backend endpoint /monitor for drift detection
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Load current data from BigQuery
    curr_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df_curr = bq.query(f"SELECT * FROM `{curr_table}` LIMIT 1000").to_dataframe()

    # Call /monitor endpoint with reference data from GCS
    response = requests.post(f"{ENV_CONFIG['API_URL']}/monitor", json={
        "reference_path": "gs://df_traffic_cyclist1/data/reference_data.csv",
        "current_data": df_curr.to_dict(orient="records"),
        "output_html": f"drift_report_{today}.html"
    })

    if response.status_code != 200:
        raise Exception(f"‚ùå Drift detection failed: {response.text}")

    result = response.json()
    drift_detected = result["drift_summary"]["drift_detected"]

    context['ti'].xcom_push(key="drift_detected", value=drift_detected)
    print(f"{'üö® Drift detected' if drift_detected else '‚úÖ No drift'}")

# 2Ô∏è‚É£ MODEL VALIDATION
def validate_model(**context):
    """
    Compare predictions vs true labels from BigQuery
    Calculate RMSE and R¬≤ for model performance
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Join predictions with actual values
    query = f"""
    SELECT
        p.prediction,
        r.`Comptage horaire` as true_value
    FROM `{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_predictions.daily_{today}` p
    JOIN `{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}` r
    ON p.`Identifiant du compteur` = r.`Identifiant du compteur`
    """

    df = bq.query(query).to_dataframe()

    from sklearn.metrics import mean_squared_error, r2_score
    import numpy as np

    rmse = np.sqrt(mean_squared_error(df['true_value'], df['prediction']))
    r2 = r2_score(df['true_value'], df['prediction'])

    context['ti'].xcom_push(key="rmse", value=rmse)
    context['ti'].xcom_push(key="r2", value=r2)

    print(f"üìä RMSE: {rmse:.2f}, R¬≤: {r2:.4f}")

# 3Ô∏è‚É£ DECISION LOGIC
def decide_if_fine_tune(**context):
    """
    Decide whether to trigger fine-tuning based on:
    - Drift detected
    - R¬≤ below threshold (0.65)
    - RMSE above threshold (60.0)
    """
    drift = context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected")
    r2 = context['ti'].xcom_pull(task_ids="validate_model", key="r2")
    rmse = context['ti'].xcom_pull(task_ids="validate_model", key="rmse")

    R2_THRESHOLD = 0.65
    RMSE_THRESHOLD = 60.0

    if drift and (r2 < R2_THRESHOLD or rmse > RMSE_THRESHOLD):
        print(f"üö® Fine-tuning needed: drift={drift}, R¬≤={r2:.4f}, RMSE={rmse:.2f}")
        return "fine_tune_model"
    else:
        print(f"‚úÖ Model OK: drift={drift}, R¬≤={r2:.4f}, RMSE={rmse:.2f}")
        return "end_monitoring"

# 4Ô∏è‚É£ FINE-TUNING VIA /train ENDPOINT
def fine_tune_model(**context):
    """
    Call /train endpoint with fine_tuning=True
    Uses latest data from BigQuery for incremental learning
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Get fresh data from BigQuery
    table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df_fresh = bq.query(f"SELECT * FROM `{table}` LIMIT 2000").to_dataframe()

    # Call /train endpoint with fine-tuning mode
    response = requests.post(f"{ENV_CONFIG['API_URL']}/train", json={
        "model_type": "rf",
        "data_source": "bigquery",
        "data": df_fresh.to_dict(orient="records"),
        "env": ENV_CONFIG['ENV'],
        "fine_tuning": True,
        "learning_rate": 0.01,
        "epochs": 10
    }, timeout=600)

    if response.status_code != 200:
        raise Exception(f"‚ùå Fine-tuning failed: {response.text}")

    result = response.json()

    # Log to BigQuery audit
    audit_df = pd.DataFrame([{
        "timestamp": datetime.utcnow(),
        "drift_detected": context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected"),
        "rmse": context['ti'].xcom_pull(task_ids="validate_model", key="rmse"),
        "r2": context['ti'].xcom_pull(task_ids="validate_model", key="r2"),
        "fine_tune_triggered": True,
        "fine_tune_success": True,
        "model_improvement": result.get("r2_improvement", 0.0),
        "env": ENV_CONFIG['ENV']
    }])

    audit_df.to_gbq(
        destination_table=f"{ENV_CONFIG['BQ_PROJECT']}.monitoring_audit.logs",
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="append",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"‚úÖ Fine-tuning completed: R¬≤ improvement = {result.get('r2_improvement', 0):.4f}")

# 5Ô∏è‚É£ END WITHOUT TRAINING
def end_monitoring(**context):
    """Log monitoring results without training"""
    audit_df = pd.DataFrame([{
        "timestamp": datetime.utcnow(),
        "drift_detected": context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected"),
        "rmse": context['ti'].xcom_pull(task_ids="validate_model", key="rmse"),
        "r2": context['ti'].xcom_pull(task_ids="validate_model", key="r2"),
        "fine_tune_triggered": False,
        "fine_tune_success": False,
        "model_improvement": 0.0,
        "env": ENV_CONFIG['ENV']
    }])

    audit_df.to_gbq(
        destination_table=f"{ENV_CONFIG['BQ_PROJECT']}.monitoring_audit.logs",
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="append",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print("‚úÖ Monitoring complete - no training needed")

# === DAG DEFINITION ===
with DAG(
    dag_id="monitor_and_fine_tune",
    schedule_interval="@weekly",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "monitoring", "drift", "training"]
) as dag:

    monitor = PythonOperator(
        task_id="monitor_drift",
        python_callable=run_drift_monitoring
    )

    validate = PythonOperator(
        task_id="validate_model",
        python_callable=validate_model
    )

    decide = BranchPythonOperator(
        task_id="decide_fine_tune",
        python_callable=decide_if_fine_tune
    )

    fine_tune = PythonOperator(
        task_id="fine_tune_model",
        python_callable=fine_tune_model
    )

    end = PythonOperator(
        task_id="end_monitoring",
        python_callable=end_monitoring,
        trigger_rule="none_failed_min_one_success"
    )

    # Pipeline flow
    monitor >> validate >> decide
    decide >> [fine_tune, end]
```

**Visualisation du DAG** :

```text
[Monitor Drift] ‚Üí [Validate Model] ‚Üí [Decide]
                                        ‚îú‚îÄ‚Üí [Fine-tune] ‚Üí [End]
                                        ‚îî‚îÄ‚Üí [End (no training)]
```

---

#### **3.6 Prometheus + Grafana (M√©triques API)**

**Instrumentation FastAPI** :

```python
# backend/regmodel/app/fastapi_app.py
from prometheus_client import Counter, Histogram, Gauge, make_asgi_app
from starlette.middleware.base import BaseHTTPMiddleware
import time

# M√©triques custom
predictions_total = Counter('predictions_total', 'Total predictions', ['model_type'])
prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency', ['model_type'])
active_models = Gauge('active_models_count', 'Cached models count')
training_total = Counter('training_total', 'Total training runs', ['model_type', 'status'])

class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        start = time.time()
        response = await call_next(request)
        duration = time.time() - start

        if request.url.path == "/predict":
            model_type = getattr(request.state, 'model_type', 'unknown')
            predictions_total.labels(model_type=model_type).inc()
            prediction_latency.labels(model_type=model_type).observe(duration)

        return response

app.add_middleware(PrometheusMiddleware)

# Endpoint m√©triques
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

@app.get("/health")
def health():
    active_models.set(len(model_cache))
    return {"status": "healthy", "cached_models": len(model_cache)}
```

**Docker Compose** :

```yaml
# docker-compose.yaml (ajout)
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus

volumes:
  prometheus_data:
  grafana_data:
```

**Configuration Prometheus** :

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'regmodel-api'
    static_configs:
      - targets: ['regmodel-backend:8000']
    metrics_path: '/metrics'
```

**Dashboard Grafana** :

- Requ√™tes/sec : `rate(predictions_total[5m])`
- Latence p50/p95/p99 : `histogram_quantile(0.95, prediction_latency_seconds)`
- Taux erreur : `rate(http_requests_total{status=~"5.."}[5m])`
- Trainings r√©ussis : `rate(training_total{status="success"}[1h])`

---

#### **3.7 S√©curit√© API**

**API Key + Rate Limiting** :

```python
# backend/regmodel/app/fastapi_app.py
from fastapi import Security, HTTPException, Depends, Request
from fastapi.security.api_key import APIKeyHeader
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

API_KEY = os.getenv("API_KEY_SECRET", "dev-key-unsafe")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(key: str = Security(api_key_header)):
    if key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return key

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@app.post("/predict", dependencies=[Depends(verify_api_key)])
@limiter.limit("100/minute")
async def predict(data: PredictRequest, request: Request):
    request.state.model_type = data.model_type
    model = get_cached_model(data.model_type, data.metric)
    y_pred = model.predict_clean(pd.DataFrame(data.records))
    return {"predictions": y_pred.tolist()}

@app.post("/train", dependencies=[Depends(verify_api_key)])
@limiter.limit("10/hour")
async def train(data: TrainRequest, request: Request):
    # Training logic with fine-tuning support
    ...
```

**Variables d'environnement** :

```bash
# backend/regmodel/.env
ENV=PROD
API_KEY_SECRET=super-secret-prod-key-2024
```

---

### **Phase 5 (Bonus) : Kubernetes** (`feat/mlops-kubernetes`)

**Si temps disponible** :

```yaml
# k8s/deployment-regmodel.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regmodel-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: regmodel
        image: europe-west1-docker.pkg.dev/datascientest-460618/cloud-run-images/regmodel-api:latest
        env:
        - name: API_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: regmodel-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: regmodel-api
```

---

## üìã Strat√©gie de branches

```text
feat/mlops-integration (branche principale)
‚îú‚îÄ‚îÄ feat/mlops-dvc-data-versioning      # Phase 2.1
‚îú‚îÄ‚îÄ feat/mlops-tests-ci                 # Phase 2.2
‚îú‚îÄ‚îÄ feat/mlops-airflow-pipeline         # Phase 3
‚îú‚îÄ‚îÄ feat/mlops-monitoring               # Phase 4
‚îî‚îÄ‚îÄ feat/mlops-kubernetes (optionnel)   # Phase 5
```

**Workflow Git** :

1. Cr√©er branche depuis `feat/mlops-integration`
2. D√©velopper feature
3. Tester localement
4. Push + merge dans `feat/mlops-integration`
5. √Ä la fin : merge `feat/mlops-integration` ‚Üí `master`

---

## üèóÔ∏è Structure finale du projet

```text
ds_traffic_cycliste1/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml                     # ‚ú® GitHub Actions
‚îú‚îÄ‚îÄ .dvc/
‚îÇ   ‚îî‚îÄ‚îÄ config                         # ‚ú® DVC config
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ reference_data.csv.dvc         # ‚ú® Pointer DVC (train)
‚îÇ   ‚îú‚îÄ‚îÄ current_data.csv.dvc           # ‚ú® Pointer DVC (prod)
‚îÇ   ‚îî‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ ml_pipeline_dag.py             # ‚ú® DAG Airflow
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml                 # ‚ú® Config Prometheus
‚îÇ   ‚îú‚îÄ‚îÄ drift_detector.py              # ‚ú® Script Evidently
‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ       ‚îî‚îÄ‚îÄ provisioning/
‚îÇ           ‚îî‚îÄ‚îÄ dashboards/
‚îÇ               ‚îî‚îÄ‚îÄ api-metrics.json   # ‚ú® Dashboard Grafana
‚îú‚îÄ‚îÄ tests/                             # ‚ú® Tests pytest
‚îÇ   ‚îú‚îÄ‚îÄ test_pipelines.py
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ test_api_regmodel.py
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ split_data_temporal.py         # ‚ú® Split ref/current
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îî‚îÄ‚îÄ regmodel/
‚îÇ       ‚îî‚îÄ‚îÄ app/
‚îÇ           ‚îî‚îÄ‚îÄ fastapi_app.py         # ‚ú® + Prometheus + API key
‚îú‚îÄ‚îÄ docker-compose.yaml                # ‚ú® + Airflow + Prometheus + Grafana
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ mlops-data-versioning.md       # ‚ú® Doc DVC
‚îÇ   ‚îú‚îÄ‚îÄ mlops-orchestration.md         # ‚ú® Doc Airflow
‚îÇ   ‚îî‚îÄ‚îÄ mlops-monitoring.md            # ‚ú® Doc Prometheus/Evidently
‚îú‚îÄ‚îÄ pytest.ini                         # ‚ú® Config pytest
‚îú‚îÄ‚îÄ MLOPS_ROADMAP.md                   # ‚ú® Ce fichier
‚îî‚îÄ‚îÄ README.md                          # ‚ú® Mis √† jour
```

---

## üìÖ Timeline (jusqu'au 7 nov)

| Phase | Branche | Dur√©e | Dates indicatives |
|-------|---------|-------|-------------------|
| 2.1 | `feat/mlops-dvc-data-versioning` | 2j | Oct 3-4 |
| 2.2 | `feat/mlops-tests-ci` | 3j | Oct 5-7 |
| 3 | `feat/mlops-airflow-pipeline` | 5j | Oct 8-12 |
| 4 | `feat/mlops-monitoring` | 6j | Oct 13-18 |
| **Buffer** | Debug, int√©gration | 5j | Oct 19-23 |
| **Doc finale** | README, pr√©sentation | 3j | Oct 24-26 |
| **R√©p√©tition** | Soutenance | 3j | Nov 4-6 |

---

## ‚úÖ Checklist finale

### Technique

- [ ] DVC configur√© + data reference/current versionn√©es
- [ ] Tests unitaires couvrent >80% du code
- [ ] CI passe sur toutes les branches
- [ ] DAG Airflow avec logique r√©entra√Ænement conditionnel
- [ ] APIs s√©curis√©es (API key + rate limit)
- [ ] Prometheus scrape m√©triques API
- [ ] Dashboards Grafana op√©rationnels
- [ ] Rapports Evidently g√©n√©r√©s automatiquement
- [ ] Docker Compose lance toute la stack

### Documentation

- [ ] README principal mis √† jour
- [ ] Doc DVC (split temporel, versioning)
- [ ] Doc Airflow (DAG, branchement, scheduling)
- [ ] Doc Monitoring (Prometheus queries, dashboards Grafana)
- [ ] Doc Evidently (drift detection, alertes)

### Pr√©sentation

- [ ] Slides de pr√©sentation (15-20 slides)
- [ ] D√©mo vid√©o de secours
- [ ] Diagramme architecture MLOps complet
- [ ] Exemples de m√©triques/dashboards

---

## üé§ Structure pr√©sentation soutenance (20 min)

1. **Contexte & objectifs** (3 min)
   - Probl√®me : pr√©diction trafic cycliste Paris
   - Stack technique : Streamlit + FastAPI + MLflow + Airflow

2. **Architecture MLOps** (5 min)
   - Sch√©ma complet : Data versioning (DVC) ‚Üí Training (Airflow) ‚Üí Deployment (Cloud Run) ‚Üí Monitoring (Prometheus/Evidently)
   - Highlight : logique r√©entra√Ænement conditionnel

3. **D√©mo live** (8 min)
   - Trigger DAG Airflow ‚Üí voir branchement retrain
   - Appel API avec m√©triques Prometheus
   - Dashboard Grafana en temps r√©el
   - Rapport Evidently drift detection

4. **D√©fis techniques & solutions** (3 min)
   - Split temporel data pour drift detection
   - Gestion cache mod√®les avec hash MD5
   - Int√©gration DVC + Airflow

5. **Q&A** (1 min)

---

## üìö Ressources

- [DVC Documentation](https://dvc.org/doc)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Prometheus Python Client](https://github.com/prometheus/client_python)
- [Evidently AI Docs](https://docs.evidentlyai.com/)
- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)

---

**Prochaine √©tape** : Cr√©er branche `feat/mlops-dvc-data-versioning` et impl√©menter DVC ! üöÄ
