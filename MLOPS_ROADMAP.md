# ğŸ¯ MLOps Roadmap â€” Bike Traffic Prediction

**Date limite soutenance** : 7 novembre 2025
**Branche principale** : `feat/mlops-integration`

---

## ğŸ“Š Ã‰tat actuel (Phases 0-1 complÃ©tÃ©es âœ…)

- âœ… ModÃ¨les ML entraÃ®nÃ©s (RF, NN)
- âœ… MLflow tracking opÃ©rationnel (dev/prod)
- âœ… Registry custom via `summary.json` GCS
- âœ… Backend FastAPI dÃ©ployÃ© sur Cloud Run (regmodel uniquement)
- âœ… Frontend Streamlit dÃ©ployÃ©
- âœ… Docker + docker-compose pour dev local
- âœ… Environnements dev/prod sÃ©parÃ©s

---

## ğŸš€ Phases MLOps Ã  implÃ©menter

### **Phase 2 : Tests, CI & Data Versioning**

#### **2.1 Data Versioning with DVC** (`feat/mlops-dvc-data-versioning`) âœ…

**Implementation completed** âœ…

ğŸ“š **Full documentation**: [docs/dvc.md](docs/dvc.md)

**Deliverables** âœ…:

- âœ… Temporal split: reference (660K rows, 69.7%) + current (288K rows, 30.3%)
- âœ… DVC tracking with GCS remote storage
- âœ… `scripts/split_data_temporal.py` implemented

---

#### **2.2 Tests unitaires + CI** (`feat/mlops-tests-ci`) âœ…

**Implementation completed** âœ…

ğŸ“š **Full documentation**:

- [docs/pytest.md](docs/pytest.md) - Complete test suite
- [docs/ci.md](docs/ci.md) - CI/CD with GitHub Actions + Codecov

**Deliverables** âœ…:

- âœ… **47 tests** passing (13 pipelines + 17 preprocessing + 11 API + 6 registry)
- âœ… **68% coverage** (app/classes: 73.42%, model_registry: 56.31%)
- âœ… GitHub Actions CI configured with **UV**
- âœ… Codecov integration active ([dashboard](https://app.codecov.io/gh/arthurcornelio88/bike-count-prediction-app))
- âœ… Coverage artifacts (HTML reports, 30 days retention)

**Files created**:

```text
tests/
â”œâ”€â”€ test_pipelines.py          âœ… 13 tests (RF, NN)
â”œâ”€â”€ test_preprocessing.py      âœ… 17 tests (transformers)
â”œâ”€â”€ test_api_regmodel.py       âœ… 11 tests (FastAPI /predict)
â”œâ”€â”€ test_model_registry.py     âœ… 6 tests (summary.json logic)
â”œâ”€â”€ conftest.py                âœ… Shared fixtures
pytest.ini                     âœ… Configuration
.github/workflows/ci.yml       âœ… GitHub Actions
.coveragerc                    âœ… Coverage config
```

---

#### **2.3 Backend API `/train` + MLflow Integration** (`feat/mlops-tests-ci`) âœ…

**Implementation completed** âœ…

ğŸ“š **Documentation**: [docs/backend.md](docs/backend.md#train---train-and-upload-model)

**Objectifs** :

- âœ… Refactor training logic into unified `train_model()` function
- âœ… Create FastAPI `/train` endpoint for remote training
- âœ… Integrate MLflow tracking in docker-compose stack
- âœ… Support DVC-tracked datasets (reference/current)
- âœ… Automatic GCS upload + `summary.json` update

**Deliverables** âœ…:

- âœ… `train_model()` function in [train.py:256](backend/regmodel/app/train.py#L256)
- âœ… `/train` endpoint in [fastapi_app.py:101](backend/regmodel/app/fastapi_app.py#L101)
- âœ… Docker Compose with RegModel API + MLflow server
- âœ… UV-optimized Dockerfile ([backend/regmodel/Dockerfile](backend/regmodel/Dockerfile))
- âœ… Dedicated pyproject.toml for RegModel service
- âœ… MLflow tracking already integrated in `train_rf()`, `train_nn()`, `train_rfc()`

**Architecture**:

```yaml
services:
  mlflow:
    - Tracking server on port 5000
    - Backend store: ./mlruns_dev
    - Artifacts: ./mlflow_artifacts
    - Healthcheck enabled

  regmodel-backend:
    - FastAPI on port 8000
    - Depends on MLflow (healthcheck)
    - Mounts: code, GCS credentials, data
    - Hot reload enabled (dev mode)
```

**API Usage**:

```bash
# Train RF model on reference data
curl -X POST "http://localhost:8000/train" \
  -H "Content-Type: application/json" \
  -d '{
    "model_type": "rf",
    "data_source": "reference",
    "env": "prod"
  }'

# Response includes: run_id, metrics, model_uri
```

**Supported models**:

- `rf`: Random Forest regressor
- `nn`: Neural Network regressor
- `rf_class`: Random Forest classifier (affluence detection)

**MÃ©triques trackÃ©es** (alignÃ© avec `summary.json`) :

- **RÃ©gression (RF, NN)** : `r2_train`, `rmse_train`
- **Classification (RFC)** : `accuracy`, `precision`, `recall`, `f1_score`
- **Hyperparams** :
  - RF: `n_estimators`, `max_depth`, `random_state`
  - NN: `embedding_dim`, `batch_size`, `epochs`, `total_params`

**Validation completed** âœ…:

- âœ… Full stack tested: `docker compose up` works
- âœ… MLflow UI accessible at <http://localhost:5000>
- âœ… `/train` endpoint tested with RF, NN models
- âœ… Test mode (`test_mode=true`) working with `test_sample.csv` (6s for NN, ~30s for RF)
- âœ… Metrics correctly returned in API response (RMSE, RÂ²)
- âœ… MLflow tracking confirmed (runs, metrics, tags, artifacts)

---

### **Phase 3 : Orchestration Airflow + Monitoring Production** (`feat/mlops-airflow-pipeline`)

**Status**: ğŸ”„ In Progress

**Objectifs unifiÃ©s** :

- ğŸ”„ Pipeline automatisÃ© end-to-end avec Airflow
- ğŸ“Š Monitoring avec BigQuery (raw, predictions, audit)
- ğŸ” Drift detection avec Evidently
- ğŸ¯ RÃ©entraÃ®nement intelligent via endpoint `/train` (fine-tuning)
- ğŸ“ˆ MÃ©triques API avec Prometheus + Grafana
- ğŸ”’ SÃ©curitÃ© API (API Key + Rate Limiting)

**Data Strategy** (Updated 2025-10-11) âœ…:

After data quality validation, we identified that all data sources (reference_data.csv,
current_data.csv, current_api_data.csv) are from the same origin (Paris Open Data historical
exports) with perfect correlation (r=1.0, MAE=0).

**Final Decision**: Use `current_api_data.csv` (905k records, 2024-09-01 â†’ 2025-10-10) as unified baseline:

- 80% Train: ~724k records (2024-09 â†’ 2025-08)
- 20% Test: ~181k records (2025-08 â†’ 2025-10)
- Live API ingestion starting 2025-10-11 (cutoff date)
- Weekly drift detection + conditional fine-tuning

ğŸ“š **Full documentation**: [docs/fetch_data_strategy.md](docs/fetch_data_strategy.md)

---

#### **3.1 Data Preparation & Baseline** âœ…

**Baseline Creation**:

```bash
# Split current_api_data.csv into train/test (80/20 split)
python scripts/split_data_temporal.py

# Output:
# - data/train_baseline.csv (~724k records, 2024-09-01 â†’ 2025-08-15)
# - data/test_baseline.csv (~181k records, 2025-08-16 â†’ 2025-10-10)
```

**GCS Upload** (baseline for champion model training):

```bash
# Upload train_baseline.csv to GCS
gsutil -m cp data/train_baseline.csv gs://<your-bucket>/data/train_baseline.csv

# Verify upload
gsutil ls -lh gs://<your-bucket>/data/
```

**DVC Tracking** (optional - for local versioning):

```bash
dvc add data/train_baseline.csv data/test_baseline.csv
dvc push
git add data/*.dvc .dvc/config
git commit -m "chore: add new baseline from current_api_data"
```

---

#### **3.1.5 Training Strategy** (Hybrid Architecture)

**Architecture**: Local champion training + Production fine-tuning

| Component | Where | When | Data Size | Duration |
|-----------|-------|------|-----------|----------|
| **Champion Training** | ğŸ’» Local | One-time (+ quarterly) | 724k records | 15-30 min |
| **Fine-Tuning** | â˜ï¸ Production | Weekly (if drift) | 2k records | 5-10 min |
| **Evaluation** | â˜ï¸ Production | Weekly | 181k test set | 2-3 min |
| **Inference** | â˜ï¸ Production | Daily | 100 records | <1 sec |

**Workflow**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INITIAL SETUP (Local - One Time)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Train champion_v1 on train_baseline.csv (local)     â”‚
â”‚ 2. Evaluate on test_baseline.csv â†’ MAE: ~12            â”‚
â”‚ 3. Upload to GCS + MLflow registry                     â”‚
â”‚ 4. Deploy to Cloud Run API                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION (Weekly DAG)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Fetch last 7 days from BigQuery                     â”‚
â”‚ 2. Drift detection (Evidently vs test_baseline)        â”‚
â”‚ 3. If NO drift â†’ skip, keep champion                   â”‚
â”‚ 4. If drift â†’ fine-tune on last 30 days                â”‚
â”‚ 5. Evaluate challenger on SAME test_baseline.csv       â”‚
â”‚ 6. Champion/Challenger decision (5% threshold)         â”‚
â”‚ 7. Log metrics to monitoring_audit.logs                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUARTERLY RETRAIN (Local - Every 3 months)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download all BigQuery data (3 months)               â”‚
â”‚ 2. Merge with train_baseline.csv â†’ new_train.csv       â”‚
â”‚ 3. Retrain champion_v2 locally (full training)         â”‚
â”‚ 4. Evaluate on SAME test_baseline.csv                  â”‚
â”‚ 5. If improved â†’ deploy as new champion                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Decisions**:

- **Local training**: Full champion model on complete baseline (724k records)
- **Production fine-tuning**: Lightweight adaptation on recent data (30 days, ~2k records)
- **Fixed test set**: Always evaluate on same test_baseline.csv for valid comparison
- **Champion/Challenger**: Promote only if 5% MAE improvement on test set

ğŸ“š **Full strategy**: [docs/training_strategy.md](docs/training_strategy.md)

---

#### **3.2 Architecture BigQuery**

**3 Datasets pour traÃ§abilitÃ© complÃ¨te** :

```yaml
# Structure BigQuery
datascientest-460618:
  bike_traffic_raw:           # DonnÃ©es brutes quotidiennes
    - daily_YYYYMMDD          # Tables par jour (comptage horaire)

  bike_traffic_predictions:   # PrÃ©dictions quotidiennes
    - daily_YYYYMMDD          # PrÃ©dictions + scores de confiance
    - prediction_ts           # Timestamp de prÃ©diction

  monitoring_audit:           # Logs de monitoring et rÃ©entraÃ®nement
    - logs                    # Audit complet (drift, AUC, fine-tuning)
```

**Schema des tables** :

```python
# bike_traffic_raw.daily_YYYYMMDD
{
    "Comptage horaire": INTEGER,
    "Date et heure de comptage": TIMESTAMP,
    "Identifiant du compteur": STRING,
    "Nom du compteur": STRING,
    "CoordonnÃ©es gÃ©ographiques": STRING,
    "ingestion_ts": TIMESTAMP
}

# bike_traffic_predictions.daily_YYYYMMDD
{
    "Comptage horaire": INTEGER,          # Valeur rÃ©elle (si disponible)
    "prediction": FLOAT,                   # PrÃ©diction du modÃ¨le
    "model_type": STRING,                  # rf, nn, rf_class
    "model_version": STRING,               # Timestamp du modÃ¨le
    "prediction_ts": TIMESTAMP
}

# monitoring_audit.logs
{
    "timestamp": TIMESTAMP,
    "drift_detected": BOOLEAN,
    "rmse": FLOAT,
    "r2": FLOAT,
    "fine_tune_triggered": BOOLEAN,
    "fine_tune_success": BOOLEAN,
    "model_improvement": FLOAT,            # Î” RÂ²
    "env": STRING,
    "error_message": STRING
}
```

---

#### **3.2 DAGs Airflow (Architecture modulaire)**

**3 DAGs sÃ©parÃ©s pour isoler les responsabilitÃ©s** :

```mermaid
graph LR
    A[dag_daily_fetch_data] -->|@daily| B[BigQuery raw]
    C[dag_daily_prediction] -->|@daily| D[BigQuery predictions]
    E[dag_monitor_and_train] -->|@weekly| F{Drift?}
    F -->|Yes| G[Evaluate Model]
    G -->|Poor RÂ²| H[Fine-tune via /train]
    G -->|Good RÂ²| I[End]
    H --> J[Update BigQuery audit]
```

**ğŸ“ Structure des fichiers** :

```text
dags/
â”œâ”€â”€ dag_daily_fetch_data.py          # Ingestion donnÃ©es brutes â†’ BigQuery
â”œâ”€â”€ dag_daily_prediction.py          # PrÃ©dictions via /predict â†’ BigQuery
â”œâ”€â”€ dag_monitor_and_train.py         # Drift + Eval + Fine-tuning
â””â”€â”€ utils/
    â”œâ”€â”€ bike_helpers.py               # Fonctions BigQuery, GCS
    â””â”€â”€ env_config.py                 # Config ENV/PROD avec Secret Manager
```

---

#### **3.3 DAG 1 : Ingestion des donnÃ©es** (`dag_daily_fetch_data.py`)

**Objectif** : RÃ©cupÃ©rer les donnÃ©es de trafic cycliste et stocker dans BigQuery

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()  # GÃ¨re DEV/PROD + Secret Manager

def fetch_bike_data_to_bq(**context):
    """
    Fetch latest bike traffic data from Paris Open Data API
    Store in BigQuery: bike_traffic_raw.daily_YYYYMMDD
    """
    today = datetime.utcnow().strftime("%Y%m%d")

    # Paris Open Data API (comptage vÃ©lo)
    api_url = "https://opendata.paris.fr/api/explore/v2.1/catalog/datasets/comptage-velo-donnees-compteurs/records"
    params = {
        "limit": 1000,
        "order_by": "date_et_heure_de_comptage DESC"
    }

    response = requests.get(api_url, params=params)
    if response.status_code != 200:
        raise Exception(f"âŒ API failed: {response.status_code}")

    data = response.json()
    df = pd.DataFrame([r['fields'] for r in data['results']])
    df["ingestion_ts"] = datetime.utcnow().isoformat()

    # Write to BigQuery
    table_id = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df.to_gbq(
        destination_table=table_id,
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="replace",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"âœ… Ingested {len(df)} records into {table_id}")

with DAG(
    dag_id="daily_fetch_bike_data",
    schedule_interval="@daily",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "ingestion", "bigquery"]
) as dag:

    fetch_task = PythonOperator(
        task_id="fetch_to_bigquery",
        python_callable=fetch_bike_data_to_bq
    )
```

---

#### **3.4 DAG 2 : PrÃ©dictions quotidiennes** (`dag_daily_prediction.py`)

**Objectif** : Lire BigQuery â†’ PrÃ©dire via `/predict` â†’ Stocker rÃ©sultats

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()

def run_daily_prediction(**context):
    """
    1. Read from BigQuery raw table
    2. Call /predict endpoint
    3. Store predictions in BigQuery predictions table
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # 1ï¸âƒ£ Read raw data
    raw_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df = bq.query(f"SELECT * FROM `{raw_table}` LIMIT 500").to_dataframe()

    # 2ï¸âƒ£ Call /predict endpoint
    api_url = f"{ENV_CONFIG['API_URL']}/predict"
    response = requests.post(api_url, json={
        "records": df.to_dict(orient="records"),
        "model_type": "rf",
        "metric": "r2"
    })

    if response.status_code != 200:
        raise Exception(f"âŒ Prediction failed: {response.text}")

    predictions = response.json()["predictions"]
    df["prediction"] = predictions
    df["model_type"] = "rf"
    df["prediction_ts"] = datetime.utcnow().isoformat()

    # 3ï¸âƒ£ Store in BigQuery
    pred_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_predictions.daily_{today}"
    df.to_gbq(
        destination_table=pred_table,
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="replace",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"âœ… Predictions saved to {pred_table}")

with DAG(
    dag_id="daily_prediction",
    schedule_interval="@daily",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "prediction", "bigquery"]
) as dag:

    predict_task = PythonOperator(
        task_id="predict_daily_data",
        python_callable=run_daily_prediction
    )
```

---

#### **3.5 DAG 3 : Monitoring + Fine-tuning** (`dag_monitor_and_train.py`)

**Objectif** : Drift detection â†’ Validation â†’ Fine-tuning conditionnel

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from google.cloud import bigquery
from utils.env_config import get_env_config

ENV_CONFIG = get_env_config()

# 1ï¸âƒ£ DRIFT DETECTION
def run_drift_monitoring(**context):
    """
    Compare reference vs current data using Evidently
    Calls backend endpoint /monitor for drift detection
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Load current data from BigQuery
    curr_table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df_curr = bq.query(f"SELECT * FROM `{curr_table}` LIMIT 1000").to_dataframe()

    # Call /monitor endpoint with reference data from GCS
    response = requests.post(f"{ENV_CONFIG['API_URL']}/monitor", json={
        "reference_path": "gs://df_traffic_cyclist1/data/reference_data.csv",
        "current_data": df_curr.to_dict(orient="records"),
        "output_html": f"drift_report_{today}.html"
    })

    if response.status_code != 200:
        raise Exception(f"âŒ Drift detection failed: {response.text}")

    result = response.json()
    drift_detected = result["drift_summary"]["drift_detected"]

    context['ti'].xcom_push(key="drift_detected", value=drift_detected)
    print(f"{'ğŸš¨ Drift detected' if drift_detected else 'âœ… No drift'}")

# 2ï¸âƒ£ MODEL VALIDATION
def validate_model(**context):
    """
    Compare predictions vs true labels from BigQuery
    Calculate RMSE and RÂ² for model performance
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Join predictions with actual values
    query = f"""
    SELECT
        p.prediction,
        r.`Comptage horaire` as true_value
    FROM `{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_predictions.daily_{today}` p
    JOIN `{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}` r
    ON p.`Identifiant du compteur` = r.`Identifiant du compteur`
    """

    df = bq.query(query).to_dataframe()

    from sklearn.metrics import mean_squared_error, r2_score
    import numpy as np

    rmse = np.sqrt(mean_squared_error(df['true_value'], df['prediction']))
    r2 = r2_score(df['true_value'], df['prediction'])

    context['ti'].xcom_push(key="rmse", value=rmse)
    context['ti'].xcom_push(key="r2", value=r2)

    print(f"ğŸ“Š RMSE: {rmse:.2f}, RÂ²: {r2:.4f}")

# 3ï¸âƒ£ DECISION LOGIC
def decide_if_fine_tune(**context):
    """
    Decide whether to trigger fine-tuning based on:
    - Drift detected
    - RÂ² below threshold (0.65)
    - RMSE above threshold (60.0)
    """
    drift = context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected")
    r2 = context['ti'].xcom_pull(task_ids="validate_model", key="r2")
    rmse = context['ti'].xcom_pull(task_ids="validate_model", key="rmse")

    R2_THRESHOLD = 0.65
    RMSE_THRESHOLD = 60.0

    if drift and (r2 < R2_THRESHOLD or rmse > RMSE_THRESHOLD):
        print(f"ğŸš¨ Fine-tuning needed: drift={drift}, RÂ²={r2:.4f}, RMSE={rmse:.2f}")
        return "fine_tune_model"
    else:
        print(f"âœ… Model OK: drift={drift}, RÂ²={r2:.4f}, RMSE={rmse:.2f}")
        return "end_monitoring"

# 4ï¸âƒ£ FINE-TUNING VIA /train ENDPOINT
def fine_tune_model(**context):
    """
    Call /train endpoint with fine_tuning=True
    Uses latest data from BigQuery for incremental learning
    """
    today = datetime.utcnow().strftime("%Y%m%d")
    bq = bigquery.Client()

    # Get fresh data from BigQuery
    table = f"{ENV_CONFIG['BQ_PROJECT']}.bike_traffic_raw.daily_{today}"
    df_fresh = bq.query(f"SELECT * FROM `{table}` LIMIT 2000").to_dataframe()

    # Call /train endpoint with fine-tuning mode
    response = requests.post(f"{ENV_CONFIG['API_URL']}/train", json={
        "model_type": "rf",
        "data_source": "bigquery",
        "data": df_fresh.to_dict(orient="records"),
        "env": ENV_CONFIG['ENV'],
        "fine_tuning": True,
        "learning_rate": 0.01,
        "epochs": 10
    }, timeout=600)

    if response.status_code != 200:
        raise Exception(f"âŒ Fine-tuning failed: {response.text}")

    result = response.json()

    # Log to BigQuery audit
    audit_df = pd.DataFrame([{
        "timestamp": datetime.utcnow(),
        "drift_detected": context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected"),
        "rmse": context['ti'].xcom_pull(task_ids="validate_model", key="rmse"),
        "r2": context['ti'].xcom_pull(task_ids="validate_model", key="r2"),
        "fine_tune_triggered": True,
        "fine_tune_success": True,
        "model_improvement": result.get("r2_improvement", 0.0),
        "env": ENV_CONFIG['ENV']
    }])

    audit_df.to_gbq(
        destination_table=f"{ENV_CONFIG['BQ_PROJECT']}.monitoring_audit.logs",
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="append",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print(f"âœ… Fine-tuning completed: RÂ² improvement = {result.get('r2_improvement', 0):.4f}")

# 5ï¸âƒ£ END WITHOUT TRAINING
def end_monitoring(**context):
    """Log monitoring results without training"""
    audit_df = pd.DataFrame([{
        "timestamp": datetime.utcnow(),
        "drift_detected": context['ti'].xcom_pull(task_ids="monitor_drift", key="drift_detected"),
        "rmse": context['ti'].xcom_pull(task_ids="validate_model", key="rmse"),
        "r2": context['ti'].xcom_pull(task_ids="validate_model", key="r2"),
        "fine_tune_triggered": False,
        "fine_tune_success": False,
        "model_improvement": 0.0,
        "env": ENV_CONFIG['ENV']
    }])

    audit_df.to_gbq(
        destination_table=f"{ENV_CONFIG['BQ_PROJECT']}.monitoring_audit.logs",
        project_id=ENV_CONFIG['BQ_PROJECT'],
        if_exists="append",
        location=ENV_CONFIG['BQ_LOCATION']
    )

    print("âœ… Monitoring complete - no training needed")

# === DAG DEFINITION ===
with DAG(
    dag_id="monitor_and_fine_tune",
    schedule_interval="@weekly",
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=["bike", "monitoring", "drift", "training"]
) as dag:

    monitor = PythonOperator(
        task_id="monitor_drift",
        python_callable=run_drift_monitoring
    )

    validate = PythonOperator(
        task_id="validate_model",
        python_callable=validate_model
    )

    decide = BranchPythonOperator(
        task_id="decide_fine_tune",
        python_callable=decide_if_fine_tune
    )

    fine_tune = PythonOperator(
        task_id="fine_tune_model",
        python_callable=fine_tune_model
    )

    end = PythonOperator(
        task_id="end_monitoring",
        python_callable=end_monitoring,
        trigger_rule="none_failed_min_one_success"
    )

    # Pipeline flow
    monitor >> validate >> decide
    decide >> [fine_tune, end]
```

**Visualisation du DAG** :

```text
[Monitor Drift] â†’ [Validate Model] â†’ [Decide]
                                        â”œâ”€â†’ [Fine-tune] â†’ [End]
                                        â””â”€â†’ [End (no training)]
```

---

#### **3.6 Prometheus + Grafana (MÃ©triques API)**

**Instrumentation FastAPI** :

```python
# backend/regmodel/app/fastapi_app.py
from prometheus_client import Counter, Histogram, Gauge, make_asgi_app
from starlette.middleware.base import BaseHTTPMiddleware
import time

# MÃ©triques custom
predictions_total = Counter('predictions_total', 'Total predictions', ['model_type'])
prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency', ['model_type'])
active_models = Gauge('active_models_count', 'Cached models count')
training_total = Counter('training_total', 'Total training runs', ['model_type', 'status'])

class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        start = time.time()
        response = await call_next(request)
        duration = time.time() - start

        if request.url.path == "/predict":
            model_type = getattr(request.state, 'model_type', 'unknown')
            predictions_total.labels(model_type=model_type).inc()
            prediction_latency.labels(model_type=model_type).observe(duration)

        return response

app.add_middleware(PrometheusMiddleware)

# Endpoint mÃ©triques
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

@app.get("/health")
def health():
    active_models.set(len(model_cache))
    return {"status": "healthy", "cached_models": len(model_cache)}
```

**Docker Compose** :

```yaml
# docker-compose.yaml (ajout)
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus

volumes:
  prometheus_data:
  grafana_data:
```

**Configuration Prometheus** :

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'regmodel-api'
    static_configs:
      - targets: ['regmodel-backend:8000']
    metrics_path: '/metrics'
```

**Dashboard Grafana** :

- RequÃªtes/sec : `rate(predictions_total[5m])`
- Latence p50/p95/p99 : `histogram_quantile(0.95, prediction_latency_seconds)`
- Taux erreur : `rate(http_requests_total{status=~"5.."}[5m])`
- Trainings rÃ©ussis : `rate(training_total{status="success"}[1h])`

---

#### **3.7 SÃ©curitÃ© API**

**API Key + Rate Limiting** :

```python
# backend/regmodel/app/fastapi_app.py
from fastapi import Security, HTTPException, Depends, Request
from fastapi.security.api_key import APIKeyHeader
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

API_KEY = os.getenv("API_KEY_SECRET", "dev-key-unsafe")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(key: str = Security(api_key_header)):
    if key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return key

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@app.post("/predict", dependencies=[Depends(verify_api_key)])
@limiter.limit("100/minute")
async def predict(data: PredictRequest, request: Request):
    request.state.model_type = data.model_type
    model = get_cached_model(data.model_type, data.metric)
    y_pred = model.predict_clean(pd.DataFrame(data.records))
    return {"predictions": y_pred.tolist()}

@app.post("/train", dependencies=[Depends(verify_api_key)])
@limiter.limit("10/hour")
async def train(data: TrainRequest, request: Request):
    # Training logic with fine-tuning support
    ...
```

**Variables d'environnement** :

```bash
# backend/regmodel/.env
ENV=PROD
API_KEY_SECRET=super-secret-prod-key-2024
```

---

### **Phase 5 (Bonus) : Kubernetes** (`feat/mlops-kubernetes`)

**Si temps disponible** :

```yaml
# k8s/deployment-regmodel.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regmodel-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: regmodel
        image: europe-west1-docker.pkg.dev/datascientest-460618/cloud-run-images/regmodel-api:latest
        env:
        - name: API_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: regmodel-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: regmodel-api
```

---

## ğŸ“‹ StratÃ©gie de branches

```text
feat/mlops-integration (branche principale)
â”œâ”€â”€ feat/mlops-dvc-data-versioning      # Phase 2.1
â”œâ”€â”€ feat/mlops-tests-ci                 # Phase 2.2
â”œâ”€â”€ feat/mlops-airflow-pipeline         # Phase 3
â”œâ”€â”€ feat/mlops-monitoring               # Phase 4
â””â”€â”€ feat/mlops-kubernetes (optionnel)   # Phase 5
```

**Workflow Git** :

1. CrÃ©er branche depuis `feat/mlops-integration`
2. DÃ©velopper feature
3. Tester localement
4. Push + merge dans `feat/mlops-integration`
5. Ã€ la fin : merge `feat/mlops-integration` â†’ `master`

---

## ğŸ—ï¸ Structure finale du projet

```text
ds_traffic_cycliste1/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                     # âœ¨ GitHub Actions
â”œâ”€â”€ .dvc/
â”‚   â””â”€â”€ config                         # âœ¨ DVC config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reference_data.csv.dvc         # âœ¨ Pointer DVC (train)
â”‚   â”œâ”€â”€ current_data.csv.dvc           # âœ¨ Pointer DVC (prod)
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ml_pipeline_dag.py             # âœ¨ DAG Airflow
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml                 # âœ¨ Config Prometheus
â”‚   â”œâ”€â”€ drift_detector.py              # âœ¨ Script Evidently
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ provisioning/
â”‚           â””â”€â”€ dashboards/
â”‚               â””â”€â”€ api-metrics.json   # âœ¨ Dashboard Grafana
â”œâ”€â”€ tests/                             # âœ¨ Tests pytest
â”‚   â”œâ”€â”€ test_pipelines.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_api_regmodel.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ split_data_temporal.py         # âœ¨ Split ref/current
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ regmodel/
â”‚       â””â”€â”€ app/
â”‚           â””â”€â”€ fastapi_app.py         # âœ¨ + Prometheus + API key
â”œâ”€â”€ docker-compose.yaml                # âœ¨ + Airflow + Prometheus + Grafana
â”œâ”€â”€ src/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ mlops-data-versioning.md       # âœ¨ Doc DVC
â”‚   â”œâ”€â”€ mlops-orchestration.md         # âœ¨ Doc Airflow
â”‚   â””â”€â”€ mlops-monitoring.md            # âœ¨ Doc Prometheus/Evidently
â”œâ”€â”€ pytest.ini                         # âœ¨ Config pytest
â”œâ”€â”€ MLOPS_ROADMAP.md                   # âœ¨ Ce fichier
â””â”€â”€ README.md                          # âœ¨ Mis Ã  jour
```

---

## ğŸ“… Timeline (jusqu'au 7 nov)

| Phase | Branche | DurÃ©e | Dates indicatives |
|-------|---------|-------|-------------------|
| 2.1 | `feat/mlops-dvc-data-versioning` | 2j | Oct 3-4 |
| 2.2 | `feat/mlops-tests-ci` | 3j | Oct 5-7 |
| 3 | `feat/mlops-airflow-pipeline` | 5j | Oct 8-12 |
| 4 | `feat/mlops-monitoring` | 6j | Oct 13-18 |
| **Buffer** | Debug, intÃ©gration | 5j | Oct 19-23 |
| **Doc finale** | README, prÃ©sentation | 3j | Oct 24-26 |
| **RÃ©pÃ©tition** | Soutenance | 3j | Nov 4-6 |

---

## âœ… Checklist finale

### Technique

- [ ] DVC configurÃ© + data reference/current versionnÃ©es
- [ ] Tests unitaires couvrent >80% du code
- [ ] CI passe sur toutes les branches
- [ ] DAG Airflow avec logique rÃ©entraÃ®nement conditionnel
- [ ] APIs sÃ©curisÃ©es (API key + rate limit)
- [ ] Prometheus scrape mÃ©triques API
- [ ] Dashboards Grafana opÃ©rationnels
- [ ] Rapports Evidently gÃ©nÃ©rÃ©s automatiquement
- [ ] Docker Compose lance toute la stack

### Documentation

- [ ] README principal mis Ã  jour
- [ ] Doc DVC (split temporel, versioning)
- [ ] Doc Airflow (DAG, branchement, scheduling)
- [ ] Doc Monitoring (Prometheus queries, dashboards Grafana)
- [ ] Doc Evidently (drift detection, alertes)

### PrÃ©sentation

- [ ] Slides de prÃ©sentation (15-20 slides)
- [ ] DÃ©mo vidÃ©o de secours
- [ ] Diagramme architecture MLOps complet
- [ ] Exemples de mÃ©triques/dashboards

---

## ğŸ¤ Structure prÃ©sentation soutenance (20 min)

1. **Contexte & objectifs** (3 min)
   - ProblÃ¨me : prÃ©diction trafic cycliste Paris
   - Stack technique : Streamlit + FastAPI + MLflow + Airflow

2. **Architecture MLOps** (5 min)
   - SchÃ©ma complet : Data versioning (DVC) â†’ Training (Airflow) â†’ Deployment (Cloud Run) â†’ Monitoring (Prometheus/Evidently)
   - Highlight : logique rÃ©entraÃ®nement conditionnel

3. **DÃ©mo live** (8 min)
   - Trigger DAG Airflow â†’ voir branchement retrain
   - Appel API avec mÃ©triques Prometheus
   - Dashboard Grafana en temps rÃ©el
   - Rapport Evidently drift detection

4. **DÃ©fis techniques & solutions** (3 min)
   - Split temporel data pour drift detection
   - Gestion cache modÃ¨les avec hash MD5
   - IntÃ©gration DVC + Airflow

5. **Q&A** (1 min)

---

## ğŸ“š Ressources

- [DVC Documentation](https://dvc.org/doc)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Prometheus Python Client](https://github.com/prometheus/client_python)
- [Evidently AI Docs](https://docs.evidentlyai.com/)
- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)

---

**Prochaine Ã©tape** : CrÃ©er branche `feat/mlops-dvc-data-versioning` et implÃ©menter DVC ! ğŸš€
