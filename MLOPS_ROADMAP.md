# ğŸ¯ MLOps Roadmap â€” Bike Traffic Prediction

**Date limite soutenance** : 7 novembre 2025
**Branche principale** : `feat/mlops-integration`

---

## ğŸ“Š Ã‰tat actuel (Phases 0-1 complÃ©tÃ©es âœ…)

- âœ… ModÃ¨les ML entraÃ®nÃ©s (RF, NN)
- âœ… MLflow tracking opÃ©rationnel (dev/prod)
- âœ… Registry custom via `summary.json` GCS
- âœ… Backend FastAPI dÃ©ployÃ© sur Cloud Run (regmodel uniquement)
- âœ… Frontend Streamlit dÃ©ployÃ©
- âœ… Docker + docker-compose pour dev local
- âœ… Environnements dev/prod sÃ©parÃ©s

---

## ğŸš€ Phases MLOps Ã  implÃ©menter

### **Phase 2 : Tests, CI & Data Versioning**

#### **2.1 Data Versioning with DVC** (`feat/mlops-dvc-data-versioning`) âœ…

**Implementation completed** âœ…

ğŸ“š **Full documentation**: [docs/dvc.md](docs/dvc.md)

**Deliverables** âœ…:

- âœ… Temporal split: reference (660K rows, 69.7%) + current (288K rows, 30.3%)
- âœ… DVC tracking with GCS remote storage
- âœ… `scripts/split_data_temporal.py` implemented

---

#### **2.2 Tests unitaires + CI** (`feat/mlops-tests-ci`) âœ…

**Implementation completed** âœ…

ğŸ“š **Full documentation**:

- [docs/pytest.md](docs/pytest.md) - Complete test suite
- [docs/ci.md](docs/ci.md) - CI/CD with GitHub Actions + Codecov

**Deliverables** âœ…:

- âœ… **47 tests** passing (13 pipelines + 17 preprocessing + 11 API + 6 registry)
- âœ… **68% coverage** (app/classes: 73.42%, model_registry: 56.31%)
- âœ… GitHub Actions CI configured with **UV**
- âœ… Codecov integration active ([dashboard](https://app.codecov.io/gh/arthurcornelio88/bike-count-prediction-app))
- âœ… Coverage artifacts (HTML reports, 30 days retention)

**Files created**:

```text
tests/
â”œâ”€â”€ test_pipelines.py          âœ… 13 tests (RF, NN)
â”œâ”€â”€ test_preprocessing.py      âœ… 17 tests (transformers)
â”œâ”€â”€ test_api_regmodel.py       âœ… 11 tests (FastAPI /predict)
â”œâ”€â”€ test_model_registry.py     âœ… 6 tests (summary.json logic)
â”œâ”€â”€ conftest.py                âœ… Shared fixtures
pytest.ini                     âœ… Configuration
.github/workflows/ci.yml       âœ… GitHub Actions
.coveragerc                    âœ… Coverage config
```

---

#### **2.3 Backend API `/train` + MLflow Integration** (`feat/mlops-tests-ci`) âœ…

**Implementation completed** âœ…

ğŸ“š **Documentation**:

- [docs/backend.md](docs/backend.md#train---train-and-upload-model)
- [docs/training_strategy.md](docs/training_strategy.md) â€” Hybrid training workflow
- [docs/mlflow_cloudsql.md](docs/mlflow_cloudsql.md) â€” Cloud SQL setup

**Objectifs** :

- âœ… Refactor training logic into unified `train_model()` function
- âœ… Create FastAPI `/train` endpoint for remote training
- âœ… Integrate MLflow tracking in docker-compose stack
- âœ… **MLflow Cloud SQL backend** for centralized team collaboration
- âœ… Support DVC-tracked datasets (reference/current)
- âœ… Automatic GCS upload + `summary.json` update

**Deliverables** âœ…:

- âœ… `train_model()` function in [train.py:256](backend/regmodel/app/train.py#L256)
- âœ… Docker Compose MLflow server + Cloud SQL Proxy
- âœ… **Cloud SQL PostgreSQL** backend (`mlflow-metadata` instance)
- âœ… UV-optimized Dockerfile ([backend/regmodel/Dockerfile](backend/regmodel/Dockerfile))
- âœ… Dedicated pyproject.toml for RegModel service
- âœ… MLflow tracking already integrated in `train_rf()`, `train_nn()`, `train_rfc()`

**Architecture**:

```yaml
services:
  cloud-sql-proxy:
    - Proxies connection to Cloud SQL PostgreSQL
    - Instance: datascientest-460618:europe-west3:mlflow-metadata
    - Requires: roles/cloudsql.client on service account

  mlflow:
    - Tracking server on port 5000
    - Backend store: Cloud SQL PostgreSQL (metadata)
    - Artifact store: gs://df_traffic_cyclist1/mlflow-artifacts/
    - Benefits: Shared team tracking, persistent, scalable
```

**Supported models**:

- `rf`: Random Forest regressor
- `nn`: Neural Network regressor
- `rf_class`: Random Forest classifier (affluence detection)

**MÃ©triques trackÃ©es** (alignÃ© avec `summary.json`) :

- **RÃ©gression (RF, NN)** : `r2_train`, `rmse_train`
- **Classification (RFC)** : `accuracy`, `precision`, `recall`, `f1_score`
- **Hyperparams** :
  - RF: `n_estimators`, `max_depth`, `random_state`
  - NN: `embedding_dim`, `batch_size`, `epochs`, `total_params`

**Validation completed** âœ…:

- âœ… MLflow stack : `docker compose up` works
- âœ… MLflow UI accessible at <http://localhost:5000>
- âœ… Cloud SQL proxy connection verified (europe-west3)
- âœ… Service account permissions configured (`roles/cloudsql.client`)
- âœ… Test mode (`test_mode=true`) working with `test_sample.csv`
- âœ… Metrics correctly returned in API response (RMSE, RÂ²)
- âœ… MLflow tracking confirmed (runs, metrics, tags, artifacts to GCS)

#### **2.4 Test `/train` Endpoint** âœ…

**Status**: âœ… Complete - Endpoint tested and working in docker-compose stack

**Quick Test:**

```bash
# Test with docker-compose stack
docker compose up -d
curl -X POST "http://localhost:8000/train" \
  -H "Content-Type: application/json" \
  -d '{"model_type":"rf","data_source":"baseline","test_mode":true,"env":"dev"}'
```

**Expected Response**: JSON with `run_id`, `metrics` (rmse, r2), and `model_uri`

**Verification Checklist:**

- âœ… `/train` endpoint working in docker-compose stack
- âœ… MLflow tracking to Cloud SQL backend
- âœ… Artifacts stored in GCS (`gs://df_traffic_cyclist1/mlflow-artifacts/`)
- âœ… `summary.json` appended to GCS
- âœ… Test mode (`test_mode=true`) working with fast training

**Ready for**: Airflow DAG 3 (`dag_monitor_and_train.py`) integration

---

### **Phase 3 : Orchestration Airflow + Monitoring Production** (`feat/mlops-airflow-pipeline`)

**Status**: ğŸ”„ In Progress (DAG 2/3 Complete âœ…)

**Progress Summary:**

- âœ… MLflow Cloud SQL backend configured (team collaboration enabled)
- âœ… Airflow stack deployed via docker-compose (WSL2 + Mac multi-platform support)
- âœ… **DAG 1/3 COMPLETE**: `dag_daily_fetch_data.py` - Data ingestion with deduplication
- âœ… **DAG 2/3 COMPLETE**: `dag_daily_prediction.py` - ML predictions with drift handling
- â³ **DAG 3/3 PENDING**: `dag_monitor_and_train.py` - Next priority
- âœ… BigQuery datasets: `bike_traffic_raw` (raw data), `bike_traffic_predictions` (predictions)
- âœ… BigQuery partitioned table architecture implemented (`comptage_velo`)
- âœ… Data drift handling in ML API (unknown compteurs fallback)
- â³ Prometheus + Grafana monitoring pending

**Objectifs unifiÃ©s** :

- ğŸ”„ Pipeline automatisÃ© end-to-end avec Airflow
- ğŸ“Š Monitoring avec BigQuery (raw, predictions, audit)
- ğŸ” Drift detection avec Evidently
- ğŸ¯ RÃ©entraÃ®nement intelligent via endpoint `/train` (fine-tuning)
- ğŸ“ˆ MÃ©triques API avec Prometheus + Grafana
- ğŸ”’ SÃ©curitÃ© API (API Key + Rate Limiting)

**Data Strategy** (Updated 2025-10-11) âœ…:

After data quality validation, we identified that all data sources (reference_data.csv,
current_data.csv, current_api_data.csv) are from the same origin (Paris Open Data historical
exports) with perfect correlation (r=1.0, MAE=0).

**Final Decision**: Use `current_api_data.csv` (905k records, 2024-09-01 â†’ 2025-10-10) as unified baseline:

- 80% Train: ~724k records (2024-09 â†’ 2025-08)
- 20% Test: ~181k records (2025-08 â†’ 2025-10)
- Live API ingestion starting 2025-10-11 (cutoff date)
- Weekly drift detection + conditional fine-tuning

ğŸ“š **Full documentation**: [docs/fetch_data_strategy.md](docs/fetch_data_strategy.md)

---

#### **3.1 Data Preparation & Baseline** âœ…

**Baseline Creation**:

```bash
# Split current_api_data.csv into train/test (80/20 split)
python scripts/split_data_temporal.py

# Output:
# - data/train_baseline.csv (~724k records, 2024-09-01 â†’ 2025-08-15)
# - data/test_baseline.csv (~181k records, 2025-08-16 â†’ 2025-10-10)
```

**GCS Upload** (baseline for champion model training):

```bash
# Upload train_baseline.csv to GCS
gsutil -m cp data/train_baseline.csv gs://<your-bucket>/data/train_baseline.csv

# Verify upload
gsutil ls -lh gs://<your-bucket>/data/
```

**DVC Tracking** (optional - for local versioning):

```bash
dvc add data/train_baseline.csv data/test_baseline.csv
dvc push
git add data/*.dvc .dvc/config
git commit -m "chore: add new baseline from current_api_data"
```

---

#### **3.1.5 Training Strategy** (Hybrid Architecture)

**Architecture**: Local champion training + Production fine-tuning

| Component | Where | When | Data Size | Duration |
|-----------|-------|------|-----------|----------|
| **Champion Training** | ğŸ’» Local | One-time (+ quarterly) | 724k records | 15-30 min |
| **Fine-Tuning** | â˜ï¸ Production | Weekly (if drift) | 2k records | 5-10 min |
| **Evaluation** | â˜ï¸ Production | Weekly | 181k test set | 2-3 min |
| **Inference** | â˜ï¸ Production | Daily | 100 records | <1 sec |

**Workflow**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INITIAL SETUP (Local - One Time)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Train champion_v1 on train_baseline.csv (local)     â”‚
â”‚ 2. Evaluate on test_baseline.csv â†’ MAE: ~12            â”‚
â”‚ 3. Upload to GCS + MLflow registry                     â”‚
â”‚ 4. Deploy to Cloud Run API                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION (Weekly DAG)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Fetch last 7 days from BigQuery                     â”‚
â”‚ 2. Drift detection (Evidently vs test_baseline)        â”‚
â”‚ 3. If NO drift â†’ skip, keep champion                   â”‚
â”‚ 4. If drift â†’ fine-tune on last 30 days                â”‚
â”‚ 5. Evaluate challenger on SAME test_baseline.csv       â”‚
â”‚ 6. Champion/Challenger decision (5% threshold)         â”‚
â”‚ 7. Log metrics to monitoring_audit.logs                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUARTERLY RETRAIN (Local - Every 3 months)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download all BigQuery data (3 months)               â”‚
â”‚ 2. Merge with train_baseline.csv â†’ new_train.csv       â”‚
â”‚ 3. Retrain champion_v2 locally (full training)         â”‚
â”‚ 4. Evaluate on SAME test_baseline.csv                  â”‚
â”‚ 5. If improved â†’ deploy as new champion                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Decisions**:

- **Local training**: Full champion model on complete baseline (724k records)
- **Production fine-tuning**: Lightweight adaptation on recent data (30 days, ~2k records)
- **Fixed test set**: Always evaluate on same test_baseline.csv for valid comparison
- **Champion/Challenger**: Promote only if 5% MAE improvement on test set

ğŸ“š **Full strategy**: [docs/training_strategy.md](docs/training_strategy.md)

---

#### **3.2 Architecture BigQuery** âœ…

**3 Datasets pour traÃ§abilitÃ© complÃ¨te** :

```yaml
# Structure BigQuery
datascientest-460618:
  bike_traffic_raw:           # âœ… DonnÃ©es brutes (IMPLEMENTED)
    - comptage_velo           # âœ… Table unique partitionnÃ©e par date

  bike_traffic_predictions:   # â³ PrÃ©dictions quotidiennes (PENDING)
    - daily_YYYYMMDD          # PrÃ©dictions + scores de confiance
    - prediction_ts           # Timestamp de prÃ©diction

  monitoring_audit:           # â³ Logs de monitoring (PENDING)
    - logs                    # Audit complet (drift, AUC, fine-tuning)
```

**Schema des tables** :

```python
# âœ… bike_traffic_raw.comptage_velo (IMPLEMENTED)
# Single partitioned table (NOT daily tables)
{
    "comptage_horaire": INTEGER,               # Hourly bike count
    "date_et_heure_de_comptage": TIMESTAMP,    # Date/time (PARTITION FIELD)
    "identifiant_du_compteur": STRING,         # Counter ID (CLUSTERING FIELD)
    "nom_du_compteur": STRING,                 # Counter name
    "latitude": FLOAT,                         # GPS latitude (extracted from coordinates)
    "longitude": FLOAT,                        # GPS longitude (extracted from coordinates)
    "ingestion_ts": TIMESTAMP                  # When record was ingested
}
# Partitioning: Daily partitions on date_et_heure_de_comptage
# Clustering: By identifiant_du_compteur for efficient queries
# Write mode: APPEND with deduplication logic

# â³ bike_traffic_predictions.daily_YYYYMMDD (PENDING - DAG 2)
{
    "comptage_horaire": INTEGER,          # Valeur rÃ©elle (si disponible)
    "prediction": FLOAT,                   # PrÃ©diction du modÃ¨le
    "model_type": STRING,                  # rf, nn, rf_class
    "model_version": STRING,               # Timestamp du modÃ¨le
    "prediction_ts": TIMESTAMP
}

# â³ monitoring_audit.logs (PENDING - DAG 3)
{
    "timestamp": TIMESTAMP,
    "drift_detected": BOOLEAN,
    "rmse": FLOAT,
    "r2": FLOAT,
    "fine_tune_triggered": BOOLEAN,
    "fine_tune_success": BOOLEAN,
    "model_improvement": FLOAT,            # Î” RÂ²
    "env": STRING,
    "error_message": STRING
}
```

**Key Architecture Decisions:**

- **Single partitioned table** instead of daily tables (better for queries, easier maintenance)
- **Deduplication logic** prevents duplicate insertions on DAG reruns
- **Clustering by counter ID** optimizes queries filtering by specific counters
- **Idempotent design** allows safe reruns without data duplication

---

#### **3.2 DAGs Airflow (Architecture modulaire)**

**3 DAGs sÃ©parÃ©s pour isoler les responsabilitÃ©s** :

```mermaid
graph LR
    A[dag_daily_fetch_data] -->|daily| B[BigQuery Raw]
    C[dag_daily_prediction] -->|daily| D[BigQuery Predictions]
    E[dag_monitor_and_train] -->|weekly| F{Drift?}
    F -->|Yes| G[Evaluate Model]
    G -->|Poor| H[Fine-tune Model]
    G -->|Good| I[End]
    H --> J[Update Audit Logs]
```

**Status**: DAG 1 âœ… Complete | DAG 2 â³ Next | DAG 3 â³ Pending

**ğŸ“ Structure des fichiers** :

```text
dags/
â”œâ”€â”€ dag_daily_fetch_data.py          # âœ… Ingestion donnÃ©es brutes â†’ BigQuery (DONE)
â”œâ”€â”€ dag_daily_prediction.py          # â³ PrÃ©dictions via /predict â†’ BigQuery (NEXT)
â”œâ”€â”€ dag_monitor_and_train.py         # â³ Drift + Eval + Fine-tuning (PENDING)
â””â”€â”€ utils/
    â”œâ”€â”€ bike_helpers.py               # âœ… Fonctions BigQuery, GCS
    â””â”€â”€ env_config.py                 # âœ… Config ENV/PROD avec Secret Manager
```

**DAG Implementation Status:**

| DAG | Status | Documentation | Tested | Notes |
|-----|--------|---------------|--------|-------|
| `dag_daily_fetch_data.py` | âœ… Complete | âœ… [docs/dags.md](docs/dags.md#1-daily-fetch-bike-data-dag) | âœ… Yes | Idempotent, deduplication, partitioned table |
| `dag_daily_prediction.py` | âœ… Complete | âœ… [docs/dags.md](docs/dags.md#2-daily-prediction-dag) | âœ… Yes | ML predictions, drift handling, RÂ²=0.79 |
| `dag_monitor_and_train.py` | â³ Pending | â³ Pending | â³ No | Drift detection + conditional fine-tuning |

---

#### **3.3 DAG 1 : Ingestion des donnÃ©es** (`dag_daily_fetch_data.py`) âœ… **COMPLETE**

**Status**: âœ… Implemented, Tested, Documented

**Objectif** : RÃ©cupÃ©rer les donnÃ©es de trafic cycliste depuis l'API Paris Open Data et stocker dans BigQuery

**Key Features Implemented:**

- âœ… API pagination (100 records/page, up to 1000 total)
- âœ… Deduplication logic (queries max existing date, filters duplicates)
- âœ… Single partitioned table (`comptage_velo`) instead of daily tables
- âœ… Data transformations (coordinates â†’ lat/lon, date â†’ TIMESTAMP)
- âœ… Idempotent design (safe to rerun multiple times)
- âœ… Validation task with graceful "no new data" handling

**Architecture:**

```text
Paris Open Data API
    â†“ (pagination: 10 pages Ã— 100 records)
fetch_to_bigquery
    â†“ (deduplication: filter existing data)
BigQuery: bike_traffic_raw.comptage_velo
    â†“ (validation: check recent ingestion)
validate_ingestion
    âœ… Success
```

**Test Results:**

```text
Run 1 (First time):
âœ… Successfully appended 1000 records to bike_traffic_raw.comptage_velo

Run 2 (Same data):
ğŸ“Š Latest data in BigQuery: 2025-10-26 22:00:00+00:00
ğŸ” Filtered out 1000 existing records (keeping 0 new records)
â„¹ï¸ No new data to ingest (all data already exists in BigQuery)
âœ… Validation passed: No new data to ingest (all data already exists)
```

**Documentation**: See [docs/dags.md](docs/dags.md) for complete implementation details

**Files Modified:**

- âœ… [dags/dag_daily_fetch_data.py](dags/dag_daily_fetch_data.py) - Complete implementation
- âœ… [dags/utils/bike_helpers.py](dags/utils/bike_helpers.py) - BigQuery helpers
- âœ… [dags/utils/env_config.py](dags/utils/env_config.py) - Environment config
- âœ… [docs/dags.md](docs/dags.md) - Full documentation with examples

---

#### **3.4 DAG 2 : PrÃ©dictions quotidiennes** (`dag_daily_prediction.py`) âœ… **COMPLETE**

**Status**: âœ… Implemented, Tested, Documented

**Objectif** : Lire BigQuery â†’ PrÃ©dire via `/predict` â†’ Stocker rÃ©sultats avec gestion du data drift

**Key Features Implemented:**

- âœ… Reads last 24h of data from partitioned table (`comptage_velo`)
- âœ… 7-day lookback for data availability check
- âœ… Data transformation for API compatibility (coordinates reconstruction)
- âœ… ML API `/predict` endpoint integration (Random Forest model)
- âœ… Data drift handling (unknown compteurs fallback)
- âœ… Prediction quality metrics (RMSE, MAE, RÂ²)
- âœ… Storage in daily prediction tables (`daily_YYYYMMDD`)
- âœ… End-to-end validation

**Test Results (2025-10-29):**

- Task 1: 2000 rows available (last 7 days)
- Task 2: 291 predictions generated (last 24h)
- Task 3: All validations passed
- Metrics: RMSE=32.70, MAE=24.17, RÂ²=0.7856 (excellent!)
- Avg prediction: 61.05 bikes/hour, Range: 37.97-401.07

**Data Drift Handling:**

Backend handles new bike counters not seen during training:

- Maps unknown compteurs to known fallback category
- Logs warnings for monitoring
- Pipeline continues without crashes
- TODO: Prometheus metrics + BigQuery audit logging

**Architecture Decisions:**

- Daily prediction tables (easier day-to-day comparison)
- Random Forest (RF) model for predictions
- Batch size: up to 500 records per run

**Documentation**: [docs/dags.md#2-daily-prediction-dag](docs/dags.md#2-daily-prediction-dag)

**Files Modified:**

- âœ… [dags/dag_daily_prediction.py](dags/dag_daily_prediction.py)
- âœ… [backend/regmodel/app/classes.py](backend/regmodel/app/classes.py)
- âœ… [docs/dags.md](docs/dags.md#2-daily-prediction-dag)

---

#### **3.5 DAG 3 : Monitoring + Fine-tuning** (`dag_monitor_and_train.py`) â³ **PENDING**

**Status**: â³ To be implemented after DAG 2

**Objectif** : Drift detection â†’ Validation â†’ Fine-tuning conditionnel

**Implementation Plan:**

1. **Drift Detection**: Use Evidently to compare reference vs current data
2. **Model Validation**: Compare predictions vs actuals from BigQuery
3. **Decision Logic**: Conditional branching based on:
   - Drift detected + RÂ² < threshold OR RMSE > threshold â†’ Fine-tune
   - Otherwise â†’ End without training
4. **Fine-tuning**: Call `/train` endpoint (already tested and working)
5. **Audit Logging**: Store all metrics in `monitoring_audit.logs`
6. **Document**: Add to [docs/dags.md](docs/dags.md) when implemented

**Architecture Flow:**

```text
[Monitor Drift] â†’ [Validate Model] â†’ [Decision]
                                        â”œâ”€â†’ [Fine-tune via /train] â†’ [Log Audit]
                                        â””â”€â†’ [Log Audit (no training)]
```

**Key Features:**

- Weekly schedule (configurable)
- BranchPythonOperator for conditional logic
- Integration with existing `/train` endpoint
- Complete audit trail in BigQuery

---

#### **3.6 Prometheus + Grafana (MÃ©triques API)** â³ **PENDING**

**Status**: â³ To be implemented

**Objectif**: Monitoring temps rÃ©el des API endpoints et modÃ¨les ML

**Implementation Plan:**

1. **FastAPI Instrumentation**:
   - Add `prometheus_client` middleware
   - Expose `/metrics` endpoint
   - Track: predictions_total, prediction_latency, active_models, training_total

2. **Docker Compose**:
   - Add Prometheus service (port 9090)
   - Add Grafana service (port 3000)
   - Configure scraping of regmodel-backend `/metrics`

3. **Grafana Dashboards**:
   - API request rate (requests/sec)
   - Prediction latency (p50, p95, p99)
   - Error rate (5xx responses)
   - Training success rate
   - Model cache size

**Access Points** (when implemented):

- Prometheus: <http://localhost:9090>
- Grafana: <http://localhost:3000> (admin/admin)

---

#### **3.7 SÃ©curitÃ© API**

**API Key + Rate Limiting** :

```python
# backend/regmodel/app/fastapi_app.py
from fastapi import Security, HTTPException, Depends, Request
from fastapi.security.api_key import APIKeyHeader
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

API_KEY = os.getenv("API_KEY_SECRET", "dev-key-unsafe")
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(key: str = Security(api_key_header)):
    if key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return key

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@app.post("/predict", dependencies=[Depends(verify_api_key)])
@limiter.limit("100/minute")
async def predict(data: PredictRequest, request: Request):
    request.state.model_type = data.model_type
    model = get_cached_model(data.model_type, data.metric)
    y_pred = model.predict_clean(pd.DataFrame(data.records))
    return {"predictions": y_pred.tolist()}

@app.post("/train", dependencies=[Depends(verify_api_key)])
@limiter.limit("10/hour")
async def train(data: TrainRequest, request: Request):
    # Training logic with fine-tuning support
    ...
```

**Variables d'environnement** :

```bash
# backend/regmodel/.env
ENV=PROD
API_KEY_SECRET=super-secret-prod-key-2024
```

---

### **Phase 5 (Bonus) : Kubernetes** (`feat/mlops-kubernetes`)

**Si temps disponible** :

```yaml
# k8s/deployment-regmodel.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regmodel-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: regmodel
        image: europe-west1-docker.pkg.dev/datascientest-460618/cloud-run-images/regmodel-api:latest
        env:
        - name: API_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: regmodel-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: regmodel-api
```

---

## ğŸ“‹ StratÃ©gie de branches

```text
feat/mlops-integration (branche principale)
â”œâ”€â”€ feat/mlops-dvc-data-versioning      # Phase 2.1
â”œâ”€â”€ feat/mlops-tests-ci                 # Phase 2.2
â”œâ”€â”€ feat/mlops-airflow-pipeline         # Phase 3
â”œâ”€â”€ feat/mlops-monitoring               # Phase 4
â””â”€â”€ feat/mlops-kubernetes (optionnel)   # Phase 5
```

**Workflow Git** :

1. CrÃ©er branche depuis `feat/mlops-integration`
2. DÃ©velopper feature
3. Tester localement
4. Push + merge dans `feat/mlops-integration`
5. Ã€ la fin : merge `feat/mlops-integration` â†’ `master`

---

## ğŸ—ï¸ Structure finale du projet

```text
ds_traffic_cycliste1/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                     # âœ¨ GitHub Actions
â”œâ”€â”€ .dvc/
â”‚   â””â”€â”€ config                         # âœ¨ DVC config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reference_data.csv.dvc         # âœ¨ Pointer DVC (train)
â”‚   â”œâ”€â”€ current_data.csv.dvc           # âœ¨ Pointer DVC (prod)
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ml_pipeline_dag.py             # âœ¨ DAG Airflow
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml                 # âœ¨ Config Prometheus
â”‚   â”œâ”€â”€ drift_detector.py              # âœ¨ Script Evidently
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ provisioning/
â”‚           â””â”€â”€ dashboards/
â”‚               â””â”€â”€ api-metrics.json   # âœ¨ Dashboard Grafana
â”œâ”€â”€ tests/                             # âœ¨ Tests pytest
â”‚   â”œâ”€â”€ test_pipelines.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_api_regmodel.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ split_data_temporal.py         # âœ¨ Split ref/current
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ regmodel/
â”‚       â””â”€â”€ app/
â”‚           â””â”€â”€ fastapi_app.py         # âœ¨ + Prometheus + API key
â”œâ”€â”€ docker-compose.yaml                # âœ¨ + Airflow + Prometheus + Grafana
â”œâ”€â”€ src/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ mlops-data-versioning.md       # âœ¨ Doc DVC
â”‚   â”œâ”€â”€ mlops-orchestration.md         # âœ¨ Doc Airflow
â”‚   â””â”€â”€ mlops-monitoring.md            # âœ¨ Doc Prometheus/Evidently
â”œâ”€â”€ pytest.ini                         # âœ¨ Config pytest
â”œâ”€â”€ MLOPS_ROADMAP.md                   # âœ¨ Ce fichier
â””â”€â”€ README.md                          # âœ¨ Mis Ã  jour
```

---

## ğŸ“… Timeline (jusqu'au 7 nov)

| Phase | Branche | DurÃ©e | Dates indicatives |
|-------|---------|-------|-------------------|
| 2.1 | `feat/mlops-dvc-data-versioning` | 2j | Oct 3-4 |
| 2.2 | `feat/mlops-tests-ci` | 3j | Oct 5-7 |
| 3 | `feat/mlops-airflow-pipeline` | 5j | Oct 8-12 |
| 4 | `feat/mlops-monitoring` | 6j | Oct 13-18 |
| **Buffer** | Debug, intÃ©gration | 5j | Oct 19-23 |
| **Doc finale** | README, prÃ©sentation | 3j | Oct 24-26 |
| **RÃ©pÃ©tition** | Soutenance | 3j | Nov 4-6 |

---

## âœ… Checklist finale

### Technique

- [ ] DVC configurÃ© + data reference/current versionnÃ©es
- [ ] Tests unitaires couvrent >80% du code
- [ ] CI passe sur toutes les branches
- [ ] DAG Airflow avec logique rÃ©entraÃ®nement conditionnel
- [ ] APIs sÃ©curisÃ©es (API key + rate limit)
- [ ] Prometheus scrape mÃ©triques API
- [ ] Dashboards Grafana opÃ©rationnels
- [ ] Rapports Evidently gÃ©nÃ©rÃ©s automatiquement
- [ ] Docker Compose lance toute la stack

### Documentation

- [ ] README principal mis Ã  jour
- [ ] Doc DVC (split temporel, versioning)
- [ ] Doc Airflow (DAG, branchement, scheduling)
- [ ] Doc Monitoring (Prometheus queries, dashboards Grafana)
- [ ] Doc Evidently (drift detection, alertes)

### PrÃ©sentation

- [ ] Slides de prÃ©sentation (15-20 slides)
- [ ] DÃ©mo vidÃ©o de secours
- [ ] Diagramme architecture MLOps complet
- [ ] Exemples de mÃ©triques/dashboards

---

## ğŸ“ˆ Recent Progress Summary (2025-10-28)

### âœ… Phase 3.3 - DAG 1 Implementation Complete

**What was accomplished:**

1. **Docker Infrastructure Fixes**:
   - Fixed multi-platform support (WSL2 amd64 + Mac arm64)
   - Resolved Airflow permission issues (logs directory)
   - Configured volume mounts for shared_data and models

2. **BigQuery Architecture**:
   - Created `bike_traffic_raw` dataset in BigQuery
   - Implemented single partitioned table (`comptage_velo`) instead of daily tables
   - Partitioning: Daily partitions on `date_et_heure_de_comptage`
   - Clustering: By `identifiant_du_compteur` for query optimization

3. **DAG 1 Implementation** ([dags/dag_daily_fetch_data.py](dags/dag_daily_fetch_data.py)):
   - API pagination handling (100 records/page, up to 1000 total)
   - Deduplication logic to prevent duplicate insertions
   - Data transformations (coordinates extraction, date conversion)
   - Validation task with graceful "no new data" handling
   - Idempotent design (safe to rerun multiple times)

4. **Documentation**:
   - Complete DAG documentation in [docs/dags.md](docs/dags.md)
   - Real log examples showing deduplication in action
   - Architecture diagrams and data flow

**Test Results:**

- âœ… First run: 1000 records ingested successfully
- âœ… Second run: 0 new records (all duplicates filtered)
- âœ… Validation passes gracefully in both cases

**Commits:**

- `2a67887` - feat: add deduplication logic to daily fetch DAG and document architecture

---

### âœ… Phase 3.4 - DAG 2 Implementation Complete (2025-10-29)

**What was accomplished:**

1. **DAG 2 Adaptation to Partitioned Tables**:
   - Modified to read from `comptage_velo` instead of daily tables
   - Added 7-day lookback check for data availability
   - Query last 24h for predictions (up to 500 records)

2. **Data Transformation Pipeline**:
   - Reconstruct `coordonnÃ©es_gÃ©ographiques` from lat/lon
   - Convert timestamps to strings for JSON serialization
   - Add pandas import for datetime detection

3. **Backend Data Drift Handling**:
   - Unknown compteurs mapped to fallback (first known category)
   - Patch loaded models for backward compatibility
   - Add logging for drift detection (TODO: Prometheus + BigQuery audit)
   - Applied to: NNPipeline, AffluenceClassifierPipeline, RFPipeline

4. **End-to-End Testing**:
   - Task 1: 2000 rows found (last 7 days)
   - Task 2: 291 predictions generated (last 24h)
   - Task 3: Validation passed
   - Metrics: RMSE=32.70, MAE=24.17, RÂ²=0.7856 (excellent!)

5. **Documentation**:
   - Complete DAG 2 documentation in [docs/dags.md#2-daily-prediction-dag](docs/dags.md#2-daily-prediction-dag)
   - Data drift handling explained with examples
   - Quality metrics table with interpretation

**Commits:**

- `5c5722a` - feat: implement DAG 2 prediction pipeline with data drift handling

**Next Steps:**

- â³ Implement DAG 3 (`dag_monitor_and_train.py`) - Monitoring + conditional retraining
- â³ Add Prometheus metrics for data drift monitoring
- â³ Add BigQuery audit table for drift events
- â³ Add Prometheus + Grafana monitoring dashboard

---

## ğŸ¤ Structure prÃ©sentation soutenance (20 min)

1. **Contexte & objectifs** (3 min)
   - ProblÃ¨me : prÃ©diction trafic cycliste Paris
   - Stack technique : Streamlit + FastAPI + MLflow + Airflow

2. **Architecture MLOps** (5 min)
   - SchÃ©ma complet : Data versioning (DVC) â†’ Training (Airflow) â†’ Deployment (Cloud Run) â†’ Monitoring (Prometheus/Evidently)
   - Highlight : logique rÃ©entraÃ®nement conditionnel

3. **DÃ©mo live** (8 min)
   - Trigger DAG Airflow â†’ voir branchement retrain
   - Appel API avec mÃ©triques Prometheus
   - Dashboard Grafana en temps rÃ©el
   - Rapport Evidently drift detection

4. **DÃ©fis techniques & solutions** (3 min)
   - Split temporel data pour drift detection
   - Gestion cache modÃ¨les avec hash MD5
   - IntÃ©gration DVC + Airflow

5. **Q&A** (1 min)

---

## ğŸ“š Ressources

- [DVC Documentation](https://dvc.org/doc)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Prometheus Python Client](https://github.com/prometheus/client_python)
- [Evidently AI Docs](https://docs.evidentlyai.com/)
- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)

---

**Prochaine Ã©tape** : CrÃ©er branche `feat/mlops-dvc-data-versioning` et implÃ©menter DVC ! ğŸš€
